{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Level - Complete Solutions\n",
    "\n",
    "This notebook contains solutions to all exercises from the Hard level lessons.\n",
    "\n",
    "## ⚠️ Important Notes\n",
    "\n",
    "- **Attempt first**: Try solving exercises before looking at solutions\n",
    "- **Multiple approaches**: Many problems have several valid solutions\n",
    "- **Understand deeply**: Don't just copy code - understand the algorithms\n",
    "- **Complexity analysis**: Pay attention to time and space complexity\n",
    "- **Test thoroughly**: Use edge cases and stress tests\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [01 - Advanced Functions and Decorators](#01-advanced-functions-and-decorators)\n",
    "2. [02 - Generators and Iterators](#02-generators-and-iterators)\n",
    "3. [03 - Algorithms and Complexity](#03-algorithms-and-complexity)\n",
    "4. [04 - Deep Learning and Neural Networks](#04-deep-learning-and-neural-networks)\n",
    "5. [05 - Advanced ML and NLP](#05-advanced-ml-and-nlp)\n",
    "6. [06 - Computer Systems and Theory](#06-computer-systems-and-theory)\n",
    "7. [08 - Classic Problems Challenges](#08-classic-problems-challenges)\n",
    "8. [09 - CTF Challenges](#09-ctf-challenges)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"01-advanced-functions-and-decorators\"></a>\n",
    "# 01 - Advanced Functions and Decorators\n",
    "\n",
    "## Exercise: Memoization Decorator\n",
    "\n",
    "Create a memoization decorator that caches function results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "import time\n",
    "\n",
    "def memoize(func):\n",
    "    \"\"\"\n",
    "    Memoization decorator with cache.\n",
    "    \n",
    "    Time: O(1) for cached results, Space: O(n) for cache\n",
    "    \"\"\"\n",
    "    cache = {}\n",
    "    \n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Create hashable key from arguments\n",
    "        # Note: This works for hashable args only (numbers, strings, tuples)\n",
    "        key = (args, tuple(sorted(kwargs.items())))\n",
    "        \n",
    "        if key in cache:\n",
    "            print(f\"Cache hit for {func.__name__}{args}\")\n",
    "            return cache[key]\n",
    "        \n",
    "        print(f\"Computing {func.__name__}{args}\")\n",
    "        result = func(*args, **kwargs)\n",
    "        cache[key] = result\n",
    "        return result\n",
    "    \n",
    "    # Expose cache for inspection\n",
    "    wrapper.cache = cache\n",
    "    wrapper.cache_clear = lambda: cache.clear()\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "# Test with Fibonacci - classic memoization example\n",
    "@memoize\n",
    "def fibonacci(n):\n",
    "    \"\"\"Calculate nth Fibonacci number.\"\"\"\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)\n",
    "\n",
    "print(\"\\n=== Testing Memoization ===\")\n",
    "print(f\"fib(10) = {fibonacci(10)}\")\n",
    "print(f\"\\nCache size: {len(fibonacci.cache)}\")\n",
    "print(f\"Cached values: {dict(list(fibonacci.cache.items())[:5])}\")\n",
    "\n",
    "# Compare performance\n",
    "def fibonacci_no_memo(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci_no_memo(n-1) + fibonacci_no_memo(n-2)\n",
    "\n",
    "print(\"\\n=== Performance Comparison ===\")\n",
    "fibonacci.cache_clear()  # Clear cache\n",
    "\n",
    "start = time.time()\n",
    "result1 = fibonacci(30)\n",
    "time1 = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "result2 = fibonacci_no_memo(30)\n",
    "time2 = time.time() - start\n",
    "\n",
    "print(f\"With memoization: {time1:.6f}s\")\n",
    "print(f\"Without memoization: {time2:.6f}s\")\n",
    "print(f\"Speedup: {time2/time1:.1f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Using functools.lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "# Python's built-in LRU cache with max size\n",
    "@lru_cache(maxsize=128)\n",
    "def fibonacci_lru(n):\n",
    "    \"\"\"Fibonacci with LRU cache (Least Recently Used).\"\"\"\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci_lru(n-1) + fibonacci_lru(n-2)\n",
    "\n",
    "print(\"Using functools.lru_cache:\")\n",
    "result = fibonacci_lru(50)\n",
    "print(f\"fib(50) = {result}\")\n",
    "print(f\"Cache info: {fibonacci_lru.cache_info()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"02-generators-and-iterators\"></a>\n",
    "# 02 - Generators and Iterators\n",
    "\n",
    "## Exercise: Generator Pipeline\n",
    "\n",
    "Create a generator pipeline that generates, filters, and transforms numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def number_generator(n):\n",
    "    \"\"\"\n",
    "    Generate n random numbers between 1 and 100.\n",
    "    \n",
    "    Space: O(1) - yields one at a time\n",
    "    \"\"\"\n",
    "    for _ in range(n):\n",
    "        yield random.randint(1, 100)\n",
    "\n",
    "def divisible_by_3_and_5(numbers):\n",
    "    \"\"\"\n",
    "    Filter numbers divisible by both 3 and 5 (i.e., divisible by 15).\n",
    "    \"\"\"\n",
    "    for num in numbers:\n",
    "        if num % 15 == 0:  # Divisible by both 3 and 5\n",
    "            yield num\n",
    "\n",
    "def multiply_by_2(numbers):\n",
    "    \"\"\"\n",
    "    Transform each number by multiplying by 2.\n",
    "    \"\"\"\n",
    "    for num in numbers:\n",
    "        yield num * 2\n",
    "\n",
    "# Complete pipeline\n",
    "def complete_pipeline(count):\n",
    "    \"\"\"\n",
    "    Complete generator pipeline:\n",
    "    Generate -> Filter (div by 15) -> Transform (multiply by 2)\n",
    "    \"\"\"\n",
    "    numbers = number_generator(count)\n",
    "    filtered = divisible_by_3_and_5(numbers)\n",
    "    transformed = multiply_by_2(filtered)\n",
    "    return transformed\n",
    "\n",
    "# Test the pipeline\n",
    "print(\"=== Generator Pipeline ===\")\n",
    "random.seed(42)  # For reproducibility\n",
    "pipeline = complete_pipeline(100)\n",
    "\n",
    "results = list(pipeline)\n",
    "print(f\"Generated 100 numbers, found {len(results)} divisible by 15\")\n",
    "print(f\"Transformed results: {results}\")\n",
    "\n",
    "# Verify: All results should be even and when divided by 2, divisible by 15\n",
    "print(\"\\nVerification:\")\n",
    "for num in results[:5]:  # Check first 5\n",
    "    original = num // 2\n",
    "    print(f\"  {num} -> original {original}, divisible by 15: {original % 15 == 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: One-liner with Generator Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact version using generator expressions\n",
    "def pipeline_compact(count):\n",
    "    \"\"\"Compact pipeline using generator expressions.\"\"\"\n",
    "    return (\n",
    "        num * 2 \n",
    "        for num in (random.randint(1, 100) for _ in range(count))\n",
    "        if num % 15 == 0\n",
    "    )\n",
    "\n",
    "random.seed(42)\n",
    "results_compact = list(pipeline_compact(100))\n",
    "print(f\"\\nCompact version results: {results_compact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Infinite Generator with Limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def infinite_random_numbers():\n",
    "    \"\"\"Infinite generator of random numbers.\"\"\"\n",
    "    while True:\n",
    "        yield random.randint(1, 100)\n",
    "\n",
    "# Take only first 10 numbers that pass the filter\n",
    "random.seed(42)\n",
    "pipeline_infinite = multiply_by_2(\n",
    "    divisible_by_3_and_5(\n",
    "        infinite_random_numbers()\n",
    "    )\n",
    ")\n",
    "\n",
    "first_10 = list(islice(pipeline_infinite, 10))\n",
    "print(f\"\\nFirst 10 from infinite pipeline: {first_10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"03-algorithms-and-complexity\"></a>\n",
    "# 03 - Algorithms and Complexity\n",
    "\n",
    "## Exercise: Dijkstra's Shortest Path Algorithm\n",
    "\n",
    "Implement Dijkstra's algorithm to find shortest paths in a weighted graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class WeightedGraph:\n",
    "    \"\"\"\n",
    "    Weighted directed graph using adjacency list.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.graph = defaultdict(list)  # {node: [(neighbor, weight), ...]}\n",
    "        self.nodes = set()\n",
    "    \n",
    "    def add_edge(self, u, v, weight):\n",
    "        \"\"\"Add directed edge from u to v with given weight.\"\"\"\n",
    "        self.graph[u].append((v, weight))\n",
    "        self.nodes.add(u)\n",
    "        self.nodes.add(v)\n",
    "    \n",
    "    def add_undirected_edge(self, u, v, weight):\n",
    "        \"\"\"Add undirected edge (bidirectional).\"\"\"\n",
    "        self.add_edge(u, v, weight)\n",
    "        self.add_edge(v, u, weight)\n",
    "    \n",
    "    def dijkstra(self, start):\n",
    "        \"\"\"\n",
    "        Dijkstra's shortest path algorithm.\n",
    "        \n",
    "        Time: O((V + E) log V) with min-heap\n",
    "        Space: O(V)\n",
    "        \n",
    "        Returns:\n",
    "            distances: Dict mapping node to shortest distance from start\n",
    "            previous: Dict mapping node to previous node in shortest path\n",
    "        \"\"\"\n",
    "        # Initialize distances to infinity\n",
    "        distances = {node: float('inf') for node in self.nodes}\n",
    "        distances[start] = 0\n",
    "        \n",
    "        # Track previous node in path for reconstruction\n",
    "        previous = {node: None for node in self.nodes}\n",
    "        \n",
    "        # Min heap: (distance, node)\n",
    "        pq = [(0, start)]\n",
    "        visited = set()\n",
    "        \n",
    "        while pq:\n",
    "            current_dist, current = heapq.heappop(pq)\n",
    "            \n",
    "            # Skip if already visited (handles duplicate entries)\n",
    "            if current in visited:\n",
    "                continue\n",
    "            \n",
    "            visited.add(current)\n",
    "            \n",
    "            # Explore neighbors\n",
    "            for neighbor, weight in self.graph[current]:\n",
    "                distance = current_dist + weight\n",
    "                \n",
    "                # Found shorter path\n",
    "                if distance < distances[neighbor]:\n",
    "                    distances[neighbor] = distance\n",
    "                    previous[neighbor] = current\n",
    "                    heapq.heappush(pq, (distance, neighbor))\n",
    "        \n",
    "        return distances, previous\n",
    "    \n",
    "    def reconstruct_path(self, previous, start, end):\n",
    "        \"\"\"\n",
    "        Reconstruct shortest path from start to end.\n",
    "        \"\"\"\n",
    "        path = []\n",
    "        current = end\n",
    "        \n",
    "        while current is not None:\n",
    "            path.append(current)\n",
    "            current = previous[current]\n",
    "        \n",
    "        path.reverse()\n",
    "        \n",
    "        # Check if path is valid\n",
    "        if path[0] == start:\n",
    "            return path\n",
    "        return []  # No path exists\n",
    "\n",
    "# Test with example graph\n",
    "print(\"=== Dijkstra's Shortest Path ===\")\n",
    "\n",
    "# Create graph\n",
    "#     (1)---4---(2)\n",
    "#     /|        /|\\\\\n",
    "#    1 |       1 | 5\n",
    "#   /  2      /  | \\\\\n",
    "# (0)  |    (4)  3  (5)\n",
    "#   \\\\  |         | /\n",
    "#    1 |         2/\n",
    "#     \\\\|        /\n",
    "#     (3)------/\n",
    "\n",
    "g = WeightedGraph()\n",
    "g.add_undirected_edge(0, 1, 1)\n",
    "g.add_undirected_edge(0, 3, 1)\n",
    "g.add_undirected_edge(1, 2, 4)\n",
    "g.add_undirected_edge(1, 3, 2)\n",
    "g.add_undirected_edge(2, 4, 1)\n",
    "g.add_undirected_edge(2, 5, 5)\n",
    "g.add_undirected_edge(3, 5, 2)\n",
    "g.add_undirected_edge(4, 5, 3)\n",
    "\n",
    "# Run Dijkstra from node 0\n",
    "start_node = 0\n",
    "distances, previous = g.dijkstra(start_node)\n",
    "\n",
    "print(f\"\\nShortest distances from node {start_node}:\")\n",
    "for node in sorted(distances.keys()):\n",
    "    dist = distances[node]\n",
    "    if dist == float('inf'):\n",
    "        print(f\"  Node {node}: unreachable\")\n",
    "    else:\n",
    "        path = g.reconstruct_path(previous, start_node, node)\n",
    "        path_str = \" -> \".join(map(str, path))\n",
    "        print(f\"  Node {node}: distance {dist}, path: {path_str}\")\n",
    "\n",
    "# Verify specific path\n",
    "end_node = 5\n",
    "path = g.reconstruct_path(previous, start_node, end_node)\n",
    "print(f\"\\n✓ Shortest path from {start_node} to {end_node}: {' -> '.join(map(str, path))}\")\n",
    "print(f\"  Total distance: {distances[end_node]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity Analysis\n",
    "\n",
    "- **Time Complexity**: O((V + E) log V)\n",
    "  - Each vertex is processed once: O(V)\n",
    "  - Each edge is relaxed once: O(E)\n",
    "  - Heap operations: O(log V)\n",
    "  \n",
    "- **Space Complexity**: O(V)\n",
    "  - Distances dict: O(V)\n",
    "  - Previous dict: O(V)\n",
    "  - Priority queue: O(V)\n",
    "  - Visited set: O(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"04-deep-learning-and-neural-networks\"></a>\n",
    "# 04 - Deep Learning and Neural Networks\n",
    "\n",
    "## Exercise: CNN for CIFAR-10\n",
    "\n",
    "Build and train a Convolutional Neural Network for CIFAR-10 image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This requires TensorFlow/Keras\n",
    "# pip install tensorflow\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, models\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "    \n",
    "    # Load CIFAR-10 dataset\n",
    "    print(\"\\n=== Loading CIFAR-10 Dataset ===\")\n",
    "    (X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape}\")\n",
    "    print(f\"Test set: {X_test.shape}\")\n",
    "    \n",
    "    # Class names\n",
    "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    \n",
    "    # Convert labels to categorical\n",
    "    y_train = keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = keras.utils.to_categorical(y_test, 10)\n",
    "    \n",
    "    # Build CNN architecture\n",
    "    print(\"\\n=== Building CNN Model ===\")\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        # Convolutional Block 1\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', \n",
    "                      input_shape=(32, 32, 3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Convolutional Block 2\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Convolutional Block 3\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Data augmentation\n",
    "    print(\"\\n=== Setting up Data Augmentation ===\")\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        zoom_range=0.1\n",
    "    )\n",
    "    datagen.fit(X_train)\n",
    "    \n",
    "    # Train model (reduced epochs for demo)\n",
    "    print(\"\\n=== Training Model ===\")\n",
    "    print(\"(Using 5 epochs for demonstration - use 50+ for production)\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        datagen.flow(X_train, y_train, batch_size=64),\n",
    "        epochs=5,  # Use 50-100 for real training\n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\n=== Final Evaluation ===\")\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test loss: {test_loss:.4f}\")\n",
    "    \n",
    "    # Make some predictions\n",
    "    predictions = model.predict(X_test[:5])\n",
    "    print(\"\\n=== Sample Predictions ===\")\n",
    "    for i in range(5):\n",
    "        pred_class = np.argmax(predictions[i])\n",
    "        true_class = np.argmax(y_test[i])\n",
    "        confidence = predictions[i][pred_class]\n",
    "        print(f\"Image {i}: Predicted '{class_names[pred_class]}' \"\n",
    "              f\"({confidence:.2%}), True: '{class_names[true_class]}'\")\n",
    "    \n",
    "    print(\"\\n✓ CNN training complete!\")\n",
    "    print(\"\\nNote: For better accuracy (80-90%), train for 50-100 epochs with:\")\n",
    "    print(\"  - Learning rate scheduling\")\n",
    "    print(\"  - More aggressive data augmentation\")\n",
    "    print(\"  - Deeper architecture (ResNet, EfficientNet)\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"TensorFlow not installed. Install with: pip install tensorflow\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nThis exercise requires TensorFlow and may take several minutes to train.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Explanation\n",
    "\n",
    "**3 Convolutional Blocks**:\n",
    "- Block 1: 32 filters, captures basic features (edges, corners)\n",
    "- Block 2: 64 filters, captures intermediate features (textures, patterns)\n",
    "- Block 3: 128 filters, captures high-level features (parts, objects)\n",
    "\n",
    "**Key Techniques**:\n",
    "- **Batch Normalization**: Stabilizes training, allows higher learning rates\n",
    "- **Dropout**: Prevents overfitting (0.2 → 0.5 progressively)\n",
    "- **Data Augmentation**: Increases effective training data\n",
    "- **Adam Optimizer**: Adaptive learning rate\n",
    "\n",
    "**Expected Performance**:\n",
    "- With 5 epochs: ~60-70% accuracy\n",
    "- With 50+ epochs: ~80-85% accuracy\n",
    "- State-of-the-art: 95%+ (with advanced architectures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"05-advanced-ml-and-nlp\"></a>\n",
    "# 05 - Advanced ML and NLP\n",
    "\n",
    "## Exercise: Advanced Text Classification\n",
    "\n",
    "Build a text classification system comparing different vectorization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text classification with multiple vectorization methods\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import re\n",
    "\n",
    "# Sample news articles dataset\n",
    "print(\"=== Text Classification System ===\")\n",
    "\n",
    "# Create sample dataset (in practice, use real news dataset like 20newsgroups)\n",
    "texts = [\n",
    "    # Technology\n",
    "    \"New AI model achieves breakthrough in natural language processing\",\n",
    "    \"Tech company releases innovative smartphone with advanced features\",\n",
    "    \"Quantum computing advances bring us closer to practical applications\",\n",
    "    \"Cybersecurity experts warn of new malware targeting businesses\",\n",
    "    \"Software update brings performance improvements to popular app\",\n",
    "    \"Machine learning algorithm predicts market trends accurately\",\n",
    "    \"Cloud computing platform expands global infrastructure\",\n",
    "    \"Developers release open-source framework for web applications\",\n",
    "    \n",
    "    # Sports\n",
    "    \"Championship team wins decisive victory in final game\",\n",
    "    \"Star athlete breaks long-standing record in track event\",\n",
    "    \"Football match ends in dramatic penalty shootout\",\n",
    "    \"Olympic gold medalist announces retirement from competition\",\n",
    "    \"Basketball team secures playoff spot with winning streak\",\n",
    "    \"Tennis player advances to finals after intense match\",\n",
    "    \"Soccer club signs international superstar in major transfer\",\n",
    "    \"Marathon runner completes race in record-breaking time\",\n",
    "    \n",
    "    # Politics\n",
    "    \"Government announces new policy on environmental protection\",\n",
    "    \"Election results show close race between candidates\",\n",
    "    \"Lawmakers debate controversial bill in heated session\",\n",
    "    \"International summit brings world leaders together\",\n",
    "    \"Political party unveils campaign strategy for upcoming election\",\n",
    "    \"Senate votes on infrastructure spending package\",\n",
    "    \"Diplomatic negotiations continue amid international tensions\",\n",
    "    \"President addresses nation on economic recovery plans\",\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech',\n",
    "    'sports', 'sports', 'sports', 'sports', 'sports', 'sports', 'sports', 'sports',\n",
    "    'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics'\n",
    "]\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Advanced text preprocessing.\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters but keep spaces\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Preprocess all texts\n",
    "texts_clean = [preprocess_text(text) for text in texts]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts_clean, labels, test_size=0.25, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset: {len(texts)} articles across 3 categories\")\n",
    "print(f\"Training: {len(X_train)}, Testing: {len(X_test)}\")\n",
    "\n",
    "# Method 1: Bag of Words (CountVectorizer)\n",
    "print(\"\\n=== Method 1: Bag of Words ===\")\n",
    "bow_vectorizer = CountVectorizer(max_features=100, stop_words='english')\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "bow_model = MultinomialNB()\n",
    "bow_model.fit(X_train_bow, y_train)\n",
    "bow_pred = bow_model.predict(X_test_bow)\n",
    "bow_acc = accuracy_score(y_test, bow_pred)\n",
    "\n",
    "print(f\"Vocabulary size: {len(bow_vectorizer.get_feature_names_out())}\")\n",
    "print(f\"Accuracy: {bow_acc:.4f}\")\n",
    "\n",
    "# Method 2: TF-IDF\n",
    "print(\"\\n=== Method 2: TF-IDF ===\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "tfidf_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "tfidf_model.fit(X_train_tfidf, y_train)\n",
    "tfidf_pred = tfidf_model.predict(X_test_tfidf)\n",
    "tfidf_acc = accuracy_score(y_test, tfidf_pred)\n",
    "\n",
    "print(f\"Accuracy: {tfidf_acc:.4f}\")\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n=== Method Comparison ===\")\n",
    "print(f\"Bag of Words:  {bow_acc:.4f}\")\n",
    "print(f\"TF-IDF:        {tfidf_acc:.4f}\")\n",
    "\n",
    "# Test on new text\n",
    "print(\"\\n=== Testing on New Articles ===\")\n",
    "new_texts = [\n",
    "    \"Revolutionary neural network architecture improves computer vision\",\n",
    "    \"National team qualifies for world championship tournament\",\n",
    "    \"Congressional hearing addresses national security concerns\"\n",
    "]\n",
    "\n",
    "for text in new_texts:\n",
    "    clean_text = preprocess_text(text)\n",
    "    vec = tfidf_vectorizer.transform([clean_text])\n",
    "    pred = tfidf_model.predict(vec)[0]\n",
    "    proba = tfidf_model.predict_proba(vec)[0]\n",
    "    confidence = max(proba)\n",
    "    print(f\"\\nText: '{text[:60]}...'\")\n",
    "    print(f\"Predicted: {pred} (confidence: {confidence:.2%})\")\n",
    "\n",
    "print(\"\\n✓ Text classification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method Comparison\n",
    "\n",
    "**Bag of Words (CountVectorizer)**:\n",
    "- Simple word frequency counts\n",
    "- Good for short texts\n",
    "- Ignores word importance\n",
    "\n",
    "**TF-IDF (Term Frequency-Inverse Document Frequency)**:\n",
    "- Weights words by importance\n",
    "- Down-weights common words\n",
    "- Better for longer documents\n",
    "\n",
    "**Word2Vec/Word Embeddings** (not shown due to training time):\n",
    "- Captures semantic meaning\n",
    "- Understands word relationships\n",
    "- Requires large training corpus\n",
    "\n",
    "For production systems, consider:\n",
    "- Use `sklearn.datasets.fetch_20newsgroups()` for real data\n",
    "- Add stemming/lemmatization with `nltk` or `spacy`\n",
    "- Try transformer models (BERT, RoBERTa) for SOTA results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"06-computer-systems-and-theory\"></a>\n",
    "# 06 - Computer Systems and Theory\n",
    "\n",
    "## Exercise: Round Robin Process Scheduler\n",
    "\n",
    "Implement a Round Robin CPU scheduling algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class Process:\n",
    "    \"\"\"Represents a process in the system.\"\"\"\n",
    "    pid: int  # Process ID\n",
    "    burst_time: int  # Total CPU time needed\n",
    "    arrival_time: int = 0  # When process arrives\n",
    "    remaining_time: int = None  # Time left to complete\n",
    "    start_time: int = None  # When first executed\n",
    "    completion_time: int = None  # When completed\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.remaining_time is None:\n",
    "            self.remaining_time = self.burst_time\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"P{self.pid}(burst={self.burst_time}, remaining={self.remaining_time})\"\n",
    "\n",
    "class RoundRobinScheduler:\n",
    "    \"\"\"\n",
    "    Round Robin CPU Scheduler.\n",
    "    \n",
    "    Each process gets a time quantum. If not finished, goes to back of queue.\n",
    "    \"\"\"\n",
    "    def __init__(self, time_quantum: int = 2):\n",
    "        self.time_quantum = time_quantum\n",
    "        self.ready_queue = deque()\n",
    "        self.current_time = 0\n",
    "        self.execution_log = []\n",
    "    \n",
    "    def add_process(self, process: Process):\n",
    "        \"\"\"Add process to ready queue.\"\"\"\n",
    "        self.ready_queue.append(process)\n",
    "    \n",
    "    def schedule(self, processes: List[Process]):\n",
    "        \"\"\"\n",
    "        Execute Round Robin scheduling.\n",
    "        \n",
    "        Time: O(n * burst_time), Space: O(n)\n",
    "        \"\"\"\n",
    "        # Sort by arrival time and add to queue\n",
    "        processes = sorted(processes, key=lambda p: p.arrival_time)\n",
    "        \n",
    "        # Add initially arrived processes\n",
    "        process_idx = 0\n",
    "        while process_idx < len(processes) and processes[process_idx].arrival_time <= self.current_time:\n",
    "            self.ready_queue.append(processes[process_idx])\n",
    "            process_idx += 1\n",
    "        \n",
    "        # Main scheduling loop\n",
    "        while self.ready_queue or process_idx < len(processes):\n",
    "            # Add newly arrived processes\n",
    "            while process_idx < len(processes) and processes[process_idx].arrival_time <= self.current_time:\n",
    "                self.ready_queue.append(processes[process_idx])\n",
    "                process_idx += 1\n",
    "            \n",
    "            # If no process ready, advance time\n",
    "            if not self.ready_queue:\n",
    "                if process_idx < len(processes):\n",
    "                    self.current_time = processes[process_idx].arrival_time\n",
    "                continue\n",
    "            \n",
    "            # Get next process\n",
    "            current_process = self.ready_queue.popleft()\n",
    "            \n",
    "            # Record start time\n",
    "            if current_process.start_time is None:\n",
    "                current_process.start_time = self.current_time\n",
    "            \n",
    "            # Execute for time quantum or remaining time\n",
    "            execution_time = min(self.time_quantum, current_process.remaining_time)\n",
    "            \n",
    "            # Log execution\n",
    "            self.execution_log.append({\n",
    "                'time': self.current_time,\n",
    "                'process': current_process.pid,\n",
    "                'duration': execution_time\n",
    "            })\n",
    "            \n",
    "            # Update process\n",
    "            current_process.remaining_time -= execution_time\n",
    "            self.current_time += execution_time\n",
    "            \n",
    "            # Add newly arrived processes before re-queuing\n",
    "            while process_idx < len(processes) and processes[process_idx].arrival_time <= self.current_time:\n",
    "                self.ready_queue.append(processes[process_idx])\n",
    "                process_idx += 1\n",
    "            \n",
    "            # Check if process completed\n",
    "            if current_process.remaining_time == 0:\n",
    "                current_process.completion_time = self.current_time\n",
    "            else:\n",
    "                # Put back in queue\n",
    "                self.ready_queue.append(current_process)\n",
    "    \n",
    "    def calculate_metrics(self, processes: List[Process]):\n",
    "        \"\"\"\n",
    "        Calculate scheduling metrics.\n",
    "        \"\"\"\n",
    "        metrics = []\n",
    "        \n",
    "        for p in processes:\n",
    "            turnaround_time = p.completion_time - p.arrival_time\n",
    "            waiting_time = turnaround_time - p.burst_time\n",
    "            response_time = p.start_time - p.arrival_time\n",
    "            \n",
    "            metrics.append({\n",
    "                'pid': p.pid,\n",
    "                'arrival': p.arrival_time,\n",
    "                'burst': p.burst_time,\n",
    "                'completion': p.completion_time,\n",
    "                'turnaround': turnaround_time,\n",
    "                'waiting': waiting_time,\n",
    "                'response': response_time\n",
    "            })\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def print_gantt_chart(self):\n",
    "        \"\"\"Print Gantt chart of execution.\"\"\"\n",
    "        print(\"\\nGantt Chart:\")\n",
    "        print(\"Time: \", end=\"\")\n",
    "        for entry in self.execution_log:\n",
    "            print(f\"{entry['time']:3d} \", end=\"\")\n",
    "        print(f\"{self.current_time:3d}\")\n",
    "        \n",
    "        print(\"Proc: \", end=\"\")\n",
    "        for entry in self.execution_log:\n",
    "            print(f\" P{entry['process']} \", end=\"\")\n",
    "        print()\n",
    "\n",
    "# Test the scheduler\n",
    "print(\"=== Round Robin Process Scheduler ===\")\n",
    "print(\"\\nTime Quantum: 2\")\n",
    "\n",
    "# Create processes\n",
    "processes = [\n",
    "    Process(pid=1, burst_time=5, arrival_time=0),\n",
    "    Process(pid=2, burst_time=3, arrival_time=1),\n",
    "    Process(pid=3, burst_time=8, arrival_time=2),\n",
    "    Process(pid=4, burst_time=6, arrival_time=3),\n",
    "]\n",
    "\n",
    "print(\"\\nProcesses:\")\n",
    "for p in processes:\n",
    "    print(f\"  P{p.pid}: Burst Time = {p.burst_time}, Arrival = {p.arrival_time}\")\n",
    "\n",
    "# Run scheduler\n",
    "scheduler = RoundRobinScheduler(time_quantum=2)\n",
    "scheduler.schedule(processes)\n",
    "\n",
    "# Print Gantt chart\n",
    "scheduler.print_gantt_chart()\n",
    "\n",
    "# Calculate and print metrics\n",
    "metrics = scheduler.calculate_metrics(processes)\n",
    "\n",
    "print(\"\\n=== Scheduling Metrics ===\")\n",
    "print(f\"{'PID':<5} {'Arrival':<8} {'Burst':<7} {'Complete':<10} {'Turnaround':<12} {'Waiting':<9} {'Response':<9}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "total_turnaround = 0\n",
    "total_waiting = 0\n",
    "total_response = 0\n",
    "\n",
    "for m in metrics:\n",
    "    print(f\"P{m['pid']:<4} {m['arrival']:<8} {m['burst']:<7} {m['completion']:<10} \"\n",
    "          f\"{m['turnaround']:<12} {m['waiting']:<9} {m['response']:<9}\")\n",
    "    total_turnaround += m['turnaround']\n",
    "    total_waiting += m['waiting']\n",
    "    total_response += m['response']\n",
    "\n",
    "n = len(metrics)\n",
    "print(\"\\n=== Averages ===\")\n",
    "print(f\"Average Turnaround Time: {total_turnaround/n:.2f}\")\n",
    "print(f\"Average Waiting Time: {total_waiting/n:.2f}\")\n",
    "print(f\"Average Response Time: {total_response/n:.2f}\")\n",
    "print(f\"\\nCPU Utilization: 100% (no idle time)\")\n",
    "print(f\"Throughput: {n/scheduler.current_time:.2f} processes/unit time\")\n",
    "\n",
    "print(\"\\n✓ Round Robin scheduling complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling Metrics Explained\n",
    "\n",
    "**Turnaround Time**: Total time from arrival to completion\n",
    "- Formula: Completion Time - Arrival Time\n",
    "- Lower is better\n",
    "\n",
    "**Waiting Time**: Time spent waiting in ready queue\n",
    "- Formula: Turnaround Time - Burst Time\n",
    "- Lower is better\n",
    "\n",
    "**Response Time**: Time from arrival to first execution\n",
    "- Formula: Start Time - Arrival Time\n",
    "- Lower is better\n",
    "\n",
    "**Round Robin Characteristics**:\n",
    "- Fair: All processes get equal time\n",
    "- Good response time: Processes don't wait long\n",
    "- Higher turnaround for long processes\n",
    "- Time quantum selection critical:\n",
    "  - Too small: High context switch overhead\n",
    "  - Too large: Degenerates to FCFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"08-classic-problems-challenges\"></a>\n",
    "# 08 - Classic Problems - Additional Challenges\n",
    "\n",
    "Solutions to the 5 challenge problems from the Classic Problems notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1: Four Sum (LeetCode #18)\n",
    "\n",
    "Extend Three Sum to find all unique quadruplets that sum to target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def four_sum(nums: list[int], target: int) -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    Find all unique quadruplets that sum to target.\n",
    "    \n",
    "    Time: O(n³), Space: O(1) excluding output\n",
    "    \n",
    "    Strategy: Sort + fix two numbers + two pointers for remaining\n",
    "    \"\"\"\n",
    "    nums.sort()\n",
    "    n = len(nums)\n",
    "    result = []\n",
    "    \n",
    "    # Fix first number\n",
    "    for i in range(n - 3):\n",
    "        # Skip duplicates for first number\n",
    "        if i > 0 and nums[i] == nums[i-1]:\n",
    "            continue\n",
    "        \n",
    "        # Fix second number\n",
    "        for j in range(i + 1, n - 2):\n",
    "            # Skip duplicates for second number\n",
    "            if j > i + 1 and nums[j] == nums[j-1]:\n",
    "                continue\n",
    "            \n",
    "            # Two pointers for remaining sum\n",
    "            left = j + 1\n",
    "            right = n - 1\n",
    "            remaining = target - nums[i] - nums[j]\n",
    "            \n",
    "            while left < right:\n",
    "                current_sum = nums[left] + nums[right]\n",
    "                \n",
    "                if current_sum == remaining:\n",
    "                    result.append([nums[i], nums[j], nums[left], nums[right]])\n",
    "                    \n",
    "                    # Skip duplicates\n",
    "                    while left < right and nums[left] == nums[left + 1]:\n",
    "                        left += 1\n",
    "                    while left < right and nums[right] == nums[right - 1]:\n",
    "                        right -= 1\n",
    "                    \n",
    "                    left += 1\n",
    "                    right -= 1\n",
    "                elif current_sum < remaining:\n",
    "                    left += 1\n",
    "                else:\n",
    "                    right -= 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test\n",
    "print(\"=== Challenge 1: Four Sum ===\")\n",
    "test_cases = [\n",
    "    ([1, 0, -1, 0, -2, 2], 0),\n",
    "    ([2, 2, 2, 2, 2], 8),\n",
    "    ([1, 2, 3, 4, 5], 10),\n",
    "]\n",
    "\n",
    "for nums, target in test_cases:\n",
    "    result = four_sum(nums, target)\n",
    "    print(f\"\\nInput: {nums}, Target: {target}\")\n",
    "    print(f\"Quadruplets: {result}\")\n",
    "\n",
    "print(\"\\n✓ Four Sum complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2: A* Search Algorithm\n",
    "\n",
    "Implement A* pathfinding with heuristic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "from typing import Tuple, List, Set, Dict\n",
    "\n",
    "def a_star_search(grid: List[List[int]], start: Tuple[int, int], \n",
    "                  goal: Tuple[int, int]) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    A* pathfinding algorithm.\n",
    "    \n",
    "    Time: O(b^d) where b is branching factor, d is depth\n",
    "    Space: O(b^d)\n",
    "    \n",
    "    Args:\n",
    "        grid: 2D grid where 0 = walkable, 1 = obstacle\n",
    "        start: Starting position (row, col)\n",
    "        goal: Goal position (row, col)\n",
    "    \n",
    "    Returns:\n",
    "        List of positions from start to goal\n",
    "    \"\"\"\n",
    "    rows, cols = len(grid), len(grid[0])\n",
    "    \n",
    "    def heuristic(pos: Tuple[int, int]) -> float:\n",
    "        \"\"\"Manhattan distance heuristic.\"\"\"\n",
    "        return abs(pos[0] - goal[0]) + abs(pos[1] - goal[1])\n",
    "    \n",
    "    def get_neighbors(pos: Tuple[int, int]) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Get valid neighbors (4-directional).\"\"\"\n",
    "        r, c = pos\n",
    "        neighbors = []\n",
    "        \n",
    "        for dr, dc in [(0, 1), (1, 0), (0, -1), (-1, 0)]:\n",
    "            nr, nc = r + dr, c + dc\n",
    "            if (0 <= nr < rows and 0 <= nc < cols and grid[nr][nc] == 0):\n",
    "                neighbors.append((nr, nc))\n",
    "        \n",
    "        return neighbors\n",
    "    \n",
    "    # Priority queue: (f_score, position)\n",
    "    # f_score = g_score + h_score\n",
    "    open_set = [(heuristic(start), start)]\n",
    "    \n",
    "    # Track best path to each node\n",
    "    came_from: Dict[Tuple, Tuple] = {}\n",
    "    \n",
    "    # g_score: actual cost from start\n",
    "    g_score = {start: 0}\n",
    "    \n",
    "    # f_score: estimated total cost\n",
    "    f_score = {start: heuristic(start)}\n",
    "    \n",
    "    # Track visited\n",
    "    closed_set: Set[Tuple] = set()\n",
    "    \n",
    "    while open_set:\n",
    "        _, current = heapq.heappop(open_set)\n",
    "        \n",
    "        # Skip if already visited\n",
    "        if current in closed_set:\n",
    "            continue\n",
    "        \n",
    "        # Goal reached!\n",
    "        if current == goal:\n",
    "            # Reconstruct path\n",
    "            path = []\n",
    "            while current in came_from:\n",
    "                path.append(current)\n",
    "                current = came_from[current]\n",
    "            path.append(start)\n",
    "            return list(reversed(path))\n",
    "        \n",
    "        closed_set.add(current)\n",
    "        \n",
    "        # Explore neighbors\n",
    "        for neighbor in get_neighbors(current):\n",
    "            if neighbor in closed_set:\n",
    "                continue\n",
    "            \n",
    "            # Cost is 1 for each step (uniform cost)\n",
    "            tentative_g = g_score[current] + 1\n",
    "            \n",
    "            # Found better path to neighbor\n",
    "            if neighbor not in g_score or tentative_g < g_score[neighbor]:\n",
    "                came_from[neighbor] = current\n",
    "                g_score[neighbor] = tentative_g\n",
    "                f_score[neighbor] = tentative_g + heuristic(neighbor)\n",
    "                heapq.heappush(open_set, (f_score[neighbor], neighbor))\n",
    "    \n",
    "    return []  # No path found\n",
    "\n",
    "# Test\n",
    "print(\"=== Challenge 2: A* Search ===\")\n",
    "\n",
    "# Create grid (0 = walkable, 1 = obstacle)\n",
    "grid = [\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 1, 0],\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "]\n",
    "\n",
    "start = (0, 0)\n",
    "goal = (4, 4)\n",
    "\n",
    "print(\"\\nGrid (S=start, G=goal, X=obstacle):\")\n",
    "for i, row in enumerate(grid):\n",
    "    for j, cell in enumerate(row):\n",
    "        if (i, j) == start:\n",
    "            print('S', end=' ')\n",
    "        elif (i, j) == goal:\n",
    "            print('G', end=' ')\n",
    "        elif cell == 1:\n",
    "            print('X', end=' ')\n",
    "        else:\n",
    "            print('.', end=' ')\n",
    "    print()\n",
    "\n",
    "path = a_star_search(grid, start, goal)\n",
    "\n",
    "print(f\"\\nPath found: {path}\")\n",
    "print(f\"Path length: {len(path)}\")\n",
    "\n",
    "# Visualize path\n",
    "print(\"\\nPath visualization:\")\n",
    "path_set = set(path)\n",
    "for i, row in enumerate(grid):\n",
    "    for j, cell in enumerate(row):\n",
    "        if (i, j) == start:\n",
    "            print('S', end=' ')\n",
    "        elif (i, j) == goal:\n",
    "            print('G', end=' ')\n",
    "        elif (i, j) in path_set:\n",
    "            print('*', end=' ')\n",
    "        elif cell == 1:\n",
    "            print('X', end=' ')\n",
    "        else:\n",
    "            print('.', end=' ')\n",
    "    print()\n",
    "\n",
    "print(\"\\n✓ A* search complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A* vs Dijkstra\n",
    "\n",
    "**Dijkstra**: \n",
    "- f(n) = g(n)\n",
    "- Explores all directions equally\n",
    "- Guaranteed optimal\n",
    "\n",
    "**A***:\n",
    "- f(n) = g(n) + h(n)\n",
    "- Uses heuristic to guide search toward goal\n",
    "- Much faster in practice\n",
    "- Optimal if heuristic is admissible (never overestimates)\n",
    "\n",
    "**Common Heuristics**:\n",
    "- Manhattan distance (4-directional movement)\n",
    "- Euclidean distance (diagonal movement)\n",
    "- Chebyshev distance (8-directional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3: SHA-256 Implementation\n",
    "\n",
    "Implement SHA-256 hash function (simplified educational version).\n",
    "\n",
    "**Note**: This is a simplified implementation for learning. Use `hashlib` in production!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, we'll show the concept and use hashlib\n",
    "import hashlib\n",
    "import struct\n",
    "\n",
    "def simple_sha256_concept(message: str) -> str:\n",
    "    \"\"\"\n",
    "    Conceptual SHA-256 (using hashlib for correct implementation).\n",
    "    \n",
    "    SHA-256 process:\n",
    "    1. Pad message to 512-bit blocks\n",
    "    2. Initialize hash values (8 constants)\n",
    "    3. Process each block:\n",
    "       - Create message schedule (64 words)\n",
    "       - Compression function (64 rounds)\n",
    "       - Update hash values\n",
    "    4. Concatenate final hash\n",
    "    \"\"\"\n",
    "    # In production, use hashlib:\n",
    "    hash_object = hashlib.sha256(message.encode())\n",
    "    return hash_object.hexdigest()\n",
    "\n",
    "print(\"=== Challenge 3: SHA-256 ===\")\n",
    "\n",
    "# Test messages\n",
    "messages = [\n",
    "    \"hello\",\n",
    "    \"Hello\",  # Capitalization changes everything\n",
    "    \"hello world\",\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"\",  # Empty string\n",
    "]\n",
    "\n",
    "print(\"\\nSHA-256 Hashes:\")\n",
    "for msg in messages:\n",
    "    hash_value = simple_sha256_concept(msg)\n",
    "    display = msg if msg else \"(empty)\"\n",
    "    print(f\"\\n'{display}'\")\n",
    "    print(f\"  {hash_value}\")\n",
    "\n",
    "# Demonstrate properties\n",
    "print(\"\\n=== SHA-256 Properties ===\")\n",
    "\n",
    "# 1. Deterministic\n",
    "h1 = simple_sha256_concept(\"test\")\n",
    "h2 = simple_sha256_concept(\"test\")\n",
    "print(f\"1. Deterministic: {h1 == h2}\")\n",
    "\n",
    "# 2. Small change → completely different hash\n",
    "h3 = simple_sha256_concept(\"test\")\n",
    "h4 = simple_sha256_concept(\"Test\")\n",
    "print(f\"2. Avalanche effect:\")\n",
    "print(f\"   'test': {h3}\")\n",
    "print(f\"   'Test': {h4}\")\n",
    "print(f\"   Different: {h3 != h4}\")\n",
    "\n",
    "# 3. Fixed output size (256 bits = 64 hex characters)\n",
    "h5 = simple_sha256_concept(\"a\")\n",
    "h6 = simple_sha256_concept(\"a\" * 1000)\n",
    "print(f\"\\n3. Fixed size: {len(h5)} hex chars (256 bits)\")\n",
    "print(f\"   Short input:  {len(h5)} chars\")\n",
    "print(f\"   Long input:   {len(h6)} chars\")\n",
    "\n",
    "# 4. One-way (computationally infeasible to reverse)\n",
    "print(f\"\\n4. One-way: Cannot reverse hash to get original message\")\n",
    "print(f\"   (Try factoring 2^256 possibilities!)\")\n",
    "\n",
    "print(\"\\n✓ SHA-256 demonstration complete!\")\n",
    "print(\"\\nNote: Full implementation requires:\")\n",
    "print(\"  - Message padding (multiple of 512 bits)\")\n",
    "print(\"  - 64 constant values (fractional parts of cube roots)\")\n",
    "print(\"  - Bitwise operations (AND, OR, XOR, ROT, SHR)\")\n",
    "print(\"  - 64 rounds of compression per block\")\n",
    "print(\"  - See FIPS 180-4 specification for details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4: Decision Tree from Scratch\n",
    "\n",
    "Implement a decision tree classifier with Gini impurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class Node:\n",
    "    \"\"\"Decision tree node.\"\"\"\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature  # Feature index to split on\n",
    "        self.threshold = threshold  # Threshold value\n",
    "        self.left = left  # Left subtree\n",
    "        self.right = right  # Right subtree\n",
    "        self.value = value  # Class value for leaf nodes\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    \"\"\"\n",
    "    Decision Tree Classifier using Gini impurity.\n",
    "    \n",
    "    Time: O(n * m * log n) for training where n=samples, m=features\n",
    "    Space: O(depth * nodes)\n",
    "    \"\"\"\n",
    "    def __init__(self, max_depth=10, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "    \n",
    "    def gini_impurity(self, y):\n",
    "        \"\"\"\n",
    "        Calculate Gini impurity.\n",
    "        \n",
    "        Gini = 1 - Σ(p_i²) where p_i is proportion of class i\n",
    "        Lower is better (0 = pure, 0.5 = maximum impurity for binary)\n",
    "        \"\"\"\n",
    "        counter = Counter(y)\n",
    "        impurity = 1.0\n",
    "        \n",
    "        for count in counter.values():\n",
    "            prob = count / len(y)\n",
    "            impurity -= prob ** 2\n",
    "        \n",
    "        return impurity\n",
    "    \n",
    "    def split(self, X, y, feature, threshold):\n",
    "        \"\"\"Split dataset based on feature and threshold.\"\"\"\n",
    "        left_mask = X[:, feature] <= threshold\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        return (X[left_mask], y[left_mask], X[right_mask], y[right_mask])\n",
    "    \n",
    "    def information_gain(self, y, y_left, y_right):\n",
    "        \"\"\"\n",
    "        Calculate information gain from split.\n",
    "        \n",
    "        Gain = Gini(parent) - weighted_average(Gini(children))\n",
    "        \"\"\"\n",
    "        parent_gini = self.gini_impurity(y)\n",
    "        \n",
    "        n = len(y)\n",
    "        n_left, n_right = len(y_left), len(y_right)\n",
    "        \n",
    "        if n_left == 0 or n_right == 0:\n",
    "            return 0\n",
    "        \n",
    "        child_gini = (n_left / n) * self.gini_impurity(y_left) + \\\n",
    "                     (n_right / n) * self.gini_impurity(y_right)\n",
    "        \n",
    "        return parent_gini - child_gini\n",
    "    \n",
    "    def best_split(self, X, y):\n",
    "        \"\"\"Find best feature and threshold to split on.\"\"\"\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        for feature in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                X_left, y_left, X_right, y_right = self.split(X, y, feature, threshold)\n",
    "                \n",
    "                gain = self.information_gain(y, y_left, y_right)\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        \"\"\"Recursively build decision tree.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        \n",
    "        # Stopping criteria\n",
    "        if (depth >= self.max_depth or \n",
    "            n_samples < self.min_samples_split or \n",
    "            n_classes == 1):\n",
    "            # Create leaf node with most common class\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "        # Find best split\n",
    "        best_feature, best_threshold = self.best_split(X, y)\n",
    "        \n",
    "        if best_feature is None:\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "        # Split and recurse\n",
    "        X_left, y_left, X_right, y_right = self.split(X, y, best_feature, best_threshold)\n",
    "        \n",
    "        left_subtree = self.build_tree(X_left, y_left, depth + 1)\n",
    "        right_subtree = self.build_tree(X_right, y_right, depth + 1)\n",
    "        \n",
    "        return Node(feature=best_feature, threshold=best_threshold,\n",
    "                   left=left_subtree, right=right_subtree)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the decision tree.\"\"\"\n",
    "        self.root = self.build_tree(X, y)\n",
    "    \n",
    "    def predict_sample(self, x, node):\n",
    "        \"\"\"Predict single sample.\"\"\"\n",
    "        if node.is_leaf():\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self.predict_sample(x, node.left)\n",
    "        else:\n",
    "            return self.predict_sample(x, node.right)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict multiple samples.\"\"\"\n",
    "        return np.array([self.predict_sample(x, self.root) for x in X])\n",
    "\n",
    "# Test\n",
    "print(\"=== Challenge 4: Decision Tree ===\")\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train our decision tree\n",
    "dt = DecisionTreeClassifier(max_depth=10, min_samples_split=2)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "predictions = dt.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"\\nDecision Tree Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nSample predictions:\")\n",
    "for i in range(5):\n",
    "    print(f\"  Predicted: {predictions[i]}, Actual: {y_test[i]}\")\n",
    "\n",
    "print(\"\\n✓ Decision tree complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 5: Topological Sort (Kahn's Algorithm)\n",
    "\n",
    "Implement topological sorting for dependency resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, defaultdict\n",
    "from typing import List, Dict\n",
    "\n",
    "def topological_sort_kahns(graph: Dict[int, List[int]], num_nodes: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Kahn's algorithm for topological sorting.\n",
    "    \n",
    "    Time: O(V + E), Space: O(V)\n",
    "    \n",
    "    Args:\n",
    "        graph: Adjacency list {node: [dependent_nodes]}\n",
    "        num_nodes: Total number of nodes\n",
    "    \n",
    "    Returns:\n",
    "        Topologically sorted order, or empty list if cycle exists\n",
    "    \"\"\"\n",
    "    # Calculate in-degree for each node\n",
    "    in_degree = {i: 0 for i in range(num_nodes)}\n",
    "    \n",
    "    for node in graph:\n",
    "        for neighbor in graph[node]:\n",
    "            in_degree[neighbor] += 1\n",
    "    \n",
    "    # Queue of nodes with no dependencies\n",
    "    queue = deque([node for node in in_degree if in_degree[node] == 0])\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        result.append(node)\n",
    "        \n",
    "        # Remove edge from graph\n",
    "        for neighbor in graph.get(node, []):\n",
    "            in_degree[neighbor] -= 1\n",
    "            \n",
    "            # If no more dependencies, add to queue\n",
    "            if in_degree[neighbor] == 0:\n",
    "                queue.append(neighbor)\n",
    "    \n",
    "    # Check for cycle\n",
    "    if len(result) != num_nodes:\n",
    "        return []  # Cycle detected\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"=== Challenge 5: Topological Sort ===\")\n",
    "\n",
    "# Example: Course prerequisites\n",
    "# Nodes represent courses, edges represent \"must take before\"\n",
    "print(\"\\nExample: Course Prerequisites\")\n",
    "print(\"Courses: 0=Intro, 1=DataStruct, 2=Algorithms, 3=ML, 4=DeepLearning, 5=NLP\")\n",
    "\n",
    "# Graph: course -> courses that depend on it\n",
    "prerequisites = {\n",
    "    0: [1],        # Intro -> DataStruct\n",
    "    1: [2],        # DataStruct -> Algorithms\n",
    "    2: [3],        # Algorithms -> ML\n",
    "    3: [4, 5],     # ML -> DeepLearning and NLP\n",
    "    4: [],\n",
    "    5: []\n",
    "}\n",
    "\n",
    "course_names = {\n",
    "    0: \"Introduction to CS\",\n",
    "    1: \"Data Structures\",\n",
    "    2: \"Algorithms\",\n",
    "    3: \"Machine Learning\",\n",
    "    4: \"Deep Learning\",\n",
    "    5: \"NLP\"\n",
    "}\n",
    "\n",
    "order = topological_sort_kahns(prerequisites, 6)\n",
    "\n",
    "if order:\n",
    "    print(\"\\nValid course order:\")\n",
    "    for i, course in enumerate(order, 1):\n",
    "        print(f\"  {i}. {course_names[course]}\")\n",
    "else:\n",
    "    print(\"\\nNo valid order - cycle detected!\")\n",
    "\n",
    "# Example: Build system dependencies\n",
    "print(\"\\n\\nExample: Build System Dependencies\")\n",
    "print(\"Modules: 0=utils, 1=config, 2=database, 3=api, 4=tests\")\n",
    "\n",
    "build_deps = {\n",
    "    0: [1, 2],     # utils used by config and database\n",
    "    1: [3],        # config used by api\n",
    "    2: [3],        # database used by api\n",
    "    3: [4],        # api used by tests\n",
    "    4: []\n",
    "}\n",
    "\n",
    "module_names = {\n",
    "    0: \"utils\",\n",
    "    1: \"config\",\n",
    "    2: \"database\",\n",
    "    3: \"api\",\n",
    "    4: \"tests\"\n",
    "}\n",
    "\n",
    "build_order = topological_sort_kahns(build_deps, 5)\n",
    "\n",
    "print(\"\\nBuild order:\")\n",
    "for i, module in enumerate(build_order, 1):\n",
    "    print(f\"  {i}. {module_names[module]}\")\n",
    "\n",
    "# Example: Cycle detection\n",
    "print(\"\\n\\nExample: Detecting Cycles\")\n",
    "cyclic_graph = {\n",
    "    0: [1],\n",
    "    1: [2],\n",
    "    2: [0],  # Cycle: 0 -> 1 -> 2 -> 0\n",
    "}\n",
    "\n",
    "result = topological_sort_kahns(cyclic_graph, 3)\n",
    "if result:\n",
    "    print(f\"Order: {result}\")\n",
    "else:\n",
    "    print(\"❌ Cycle detected! No valid topological order.\")\n",
    "\n",
    "print(\"\\n✓ Topological sort complete!\")\n",
    "\n",
    "print(\"\\n=== Applications ===\")\n",
    "print(\"- Task scheduling with dependencies\")\n",
    "print(\"- Build systems (make, npm, cargo)\")\n",
    "print(\"- Course prerequisites\")\n",
    "print(\"- Package managers (apt, pip)\")\n",
    "print(\"- Compilation order\")\n",
    "print(\"- Job scheduling in databases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"09-ctf-challenges\"></a>\n",
    "# 09 - CTF Challenges\n",
    "\n",
    "## Important Note\n",
    "\n",
    "The CTF challenges notebook (09_ctf_challenges.ipynb) contains **hands-on security exercises** that are best completed interactively. The challenges cover:\n",
    "\n",
    "- **Web**: SQL injection, XSS\n",
    "- **Binary**: Buffer overflows, format strings\n",
    "- **Reverse Engineering**: Decompiling, crackmes\n",
    "- **Cryptography**: Breaking ciphers\n",
    "- **Forensics**: Steganography, file carving\n",
    "- **OSINT**: Metadata extraction\n",
    "\n",
    "These challenges are designed to be **worked through step-by-step** with explanations in the original notebook. Rather than providing complete solutions here (which would defeat the learning purpose), we recommend:\n",
    "\n",
    "1. **Read each challenge carefully**\n",
    "2. **Attempt it yourself first**\n",
    "3. **Use the hints provided**\n",
    "4. **Research the vulnerability/technique**\n",
    "5. **Consult CTF writeups** if stuck\n",
    "\n",
    "### Additional CTF Resources\n",
    "\n",
    "**Practice Platforms**:\n",
    "- PicoCTF (beginner-friendly)\n",
    "- HackTheBox\n",
    "- TryHackMe\n",
    "- OverTheWire Wargames\n",
    "\n",
    "**Learning Resources**:\n",
    "- CTFtime.org (calendar and writeups)\n",
    "- LiveOverflow YouTube channel\n",
    "- OWASP Top 10\n",
    "\n",
    "**Note**: All techniques should only be used legally and ethically:\n",
    "- ✅ Authorized penetration testing\n",
    "- ✅ CTF competitions\n",
    "- ✅ Your own systems\n",
    "- ❌ Unauthorized access\n",
    "- ❌ Malicious use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations on completing the Hard level solutions! You've implemented:\n",
    "\n",
    "### Algorithms & Data Structures\n",
    "- Advanced decorators with memoization\n",
    "- Generator pipelines\n",
    "- Dijkstra's shortest path\n",
    "- A* search with heuristics\n",
    "- Topological sort (Kahn's algorithm)\n",
    "- Decision trees from scratch\n",
    "\n",
    "### Machine Learning & Deep Learning\n",
    "- CNN for CIFAR-10 image classification\n",
    "- Text classification with multiple vectorization methods\n",
    "- Advanced preprocessing and feature engineering\n",
    "\n",
    "### Systems & Theory\n",
    "- Round Robin CPU scheduler\n",
    "- Process scheduling metrics\n",
    "- Cryptography (SHA-256 concepts)\n",
    "\n",
    "### Problem-Solving Patterns\n",
    "- Hash maps for optimization\n",
    "- Two pointers technique\n",
    "- Dynamic programming\n",
    "- Graph algorithms\n",
    "- Greedy algorithms\n",
    "- Divide and conquer\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Practice regularly**: Solve 1-2 problems daily on LeetCode/Codeforces\n",
    "2. **Build projects**: Apply these algorithms to real-world problems\n",
    "3. **Read research papers**: Stay current with SOTA techniques\n",
    "4. **Participate in competitions**: Kaggle, CTFs, coding contests\n",
    "5. **Contribute to open source**: Real-world experience\n",
    "\n",
    "## Mastery Checklist\n",
    "\n",
    "You've mastered Hard level when you can:\n",
    "- [ ] Solve LeetCode Hard problems in 45 minutes\n",
    "- [ ] Explain algorithm complexity intuitively\n",
    "- [ ] Implement ML algorithms from scratch\n",
    "- [ ] Design system architectures\n",
    "- [ ] Debug complex concurrent systems\n",
    "- [ ] Optimize for both time and space\n",
    "- [ ] Teach concepts to others clearly\n",
    "\n",
    "**Remember**: These are advanced topics that take years to master. Be patient with yourself and enjoy the journey! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
