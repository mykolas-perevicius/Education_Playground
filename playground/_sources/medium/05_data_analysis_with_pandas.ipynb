{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5: Data Analysis with Pandas\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Pandas is Python's powerhouse library for data analysis. Think of it as Excel on steroids - it handles data manipulation, cleaning, analysis, and visualization with elegant, readable code.\n",
    "\n",
    "**Why Pandas Matters:**\n",
    "- **Industry Standard**: Used by data scientists worldwide\n",
    "- **Powerful**: Handle millions of rows efficiently\n",
    "- **Flexible**: Read from CSV, Excel, SQL, JSON, and more\n",
    "- **Integrated**: Works seamlessly with NumPy, Matplotlib, scikit-learn\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Series and DataFrames fundamentals\n",
    "- Loading and saving data (CSV, Excel, JSON)\n",
    "- Data exploration and inspection\n",
    "- Selecting, filtering, and indexing\n",
    "- Data cleaning and handling missing values\n",
    "- Grouping and aggregation\n",
    "- Merging and joining datasets\n",
    "- Time series basics\n",
    "- Data visualization\n",
    "\n",
    "**Prerequisites**: `pip install pandas numpy matplotlib openpyxl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pandas Fundamentals\n",
    "\n",
    "### Series: One-Dimensional Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a Series\n",
    "temperatures = pd.Series([72, 75, 78, 74, 71], \n",
    "                         index=['Mon', 'Tue', 'Wed', 'Thu', 'Fri'],\n",
    "                         name='Temperature')\n",
    "\n",
    "print(\"Series:\")\n",
    "print(temperatures)\n",
    "print(f\"\\nAccess by label: Wed = {temperatures['Wed']}\")\n",
    "print(f\"Access by position: [1] = {temperatures.iloc[1]}\")\n",
    "print(f\"\\nMean: {temperatures.mean():.1f}\")\n",
    "print(f\"Max: {temperatures.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames: Two-Dimensional Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from dictionary\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'Age': [25, 30, 35, 28, 32],\n",
    "    'City': ['NYC', 'Paris', 'London', 'Tokyo', 'Berlin'],\n",
    "    'Salary': [70000, 80000, 75000, 90000, 85000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"DataFrame:\")\n",
    "print(df)\n",
    "print(f\"\\nShape: {df.shape} (rows, columns)\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"Index: {df.index.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and Saving Data\n",
    "\n",
    "### Reading CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample CSV data\n",
    "import io\n",
    "\n",
    "csv_data = \"\"\"Name,Age,Department,Salary\n",
    "Alice,25,Engineering,75000\n",
    "Bob,30,Marketing,65000\n",
    "Charlie,35,Engineering,85000\n",
    "Diana,28,Sales,70000\n",
    "Eve,32,Marketing,72000\"\"\"\n",
    "\n",
    "# Read from string (simulating file read)\n",
    "df = pd.read_csv(io.StringIO(csv_data))\n",
    "print(\"Data loaded from CSV:\")\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "# df.to_csv('employees.csv', index=False)\n",
    "\n",
    "# Read with options\n",
    "# df = pd.read_csv('data.csv', \n",
    "#                  sep=',',           # Delimiter\n",
    "#                  header=0,          # Row to use as column names\n",
    "#                  na_values=['NA', 'missing'],  # Values to treat as NaN\n",
    "#                  parse_dates=['Date']  # Parse date columns\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Excel Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Excel (requires openpyxl or xlrd)\n",
    "# df = pd.read_excel('data.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# Write to Excel\n",
    "# df.to_excel('output.xlsx', sheet_name='Results', index=False)\n",
    "\n",
    "# Multiple sheets\n",
    "# with pd.ExcelWriter('multi_sheet.xlsx') as writer:\n",
    "#     df1.to_excel(writer, sheet_name='Sales')\n",
    "#     df2.to_excel(writer, sheet_name='Inventory')\n",
    "\n",
    "print(\"Excel I/O examples shown (commented out)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON and Other Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON\n",
    "json_str = '{\"Name\": [\"Alice\", \"Bob\"], \"Age\": [25, 30]}'\n",
    "df_json = pd.read_json(io.StringIO(json_str))\n",
    "print(\"From JSON:\")\n",
    "print(df_json)\n",
    "\n",
    "# To JSON\n",
    "json_output = df_json.to_json(orient='records')\n",
    "print(f\"\\nTo JSON: {json_output}\")\n",
    "\n",
    "# Other formats:\n",
    "# pd.read_sql(query, connection)  # SQL database\n",
    "# pd.read_html(url)  # HTML tables\n",
    "# pd.read_clipboard()  # From clipboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration\n",
    "\n",
    "### Inspecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'Product': np.random.choice(['Laptop', 'Phone', 'Tablet'], 100),\n",
    "    'Price': np.random.randint(200, 1500, 100),\n",
    "    'Quantity': np.random.randint(1, 10, 100),\n",
    "    'Rating': np.random.uniform(3.0, 5.0, 100)\n",
    "})\n",
    "\n",
    "# Basic info\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nLast 3 rows:\")\n",
    "print(df.tail(3))\n",
    "\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nValue Counts:\")\n",
    "print(df['Product'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Selecting and Filtering Data\n",
    "\n",
    "### Column Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single column (returns Series)\n",
    "prices = df['Price']\n",
    "print(\"Type:\", type(prices))\n",
    "print(prices.head())\n",
    "\n",
    "# Multiple columns (returns DataFrame)\n",
    "subset = df[['Product', 'Price']]\n",
    "print(\"\\nMultiple columns:\")\n",
    "print(subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row Selection: loc vs iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc: Integer-based position\n",
    "print(\"First row (iloc):\")\n",
    "print(df.iloc[0])\n",
    "\n",
    "print(\"\\nFirst 3 rows, first 2 columns:\")\n",
    "print(df.iloc[0:3, 0:2])\n",
    "\n",
    "# loc: Label-based\n",
    "df_indexed = df.set_index('Product')\n",
    "print(\"\\nSelect 'Laptop' rows (loc):\")\n",
    "print(df_indexed.loc['Laptop'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple condition\n",
    "expensive = df[df['Price'] > 1000]\n",
    "print(f\"Expensive items (>{1000}): {len(expensive)} rows\")\n",
    "print(expensive.head())\n",
    "\n",
    "# Multiple conditions (AND: &, OR: |)\n",
    "high_value = df[(df['Price'] > 800) & (df['Rating'] >= 4.5)]\n",
    "print(f\"\\nHigh value items: {len(high_value)} rows\")\n",
    "\n",
    "# Using isin()\n",
    "mobile_devices = df[df['Product'].isin(['Phone', 'Tablet'])]\n",
    "print(f\"\\nMobile devices: {len(mobile_devices)} rows\")\n",
    "\n",
    "# String methods\n",
    "# phones = df[df['Product'].str.contains('Phone')]\n",
    "# uppercase = df[df['Product'].str.isupper()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Manipulation\n",
    "\n",
    "### Adding and Modifying Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column\n",
    "df['Revenue'] = df['Price'] * df['Quantity']\n",
    "\n",
    "# Conditional column\n",
    "df['Price_Category'] = pd.cut(df['Price'], \n",
    "                               bins=[0, 500, 1000, 2000],\n",
    "                               labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Apply function\n",
    "df['Discount_Price'] = df['Price'].apply(lambda x: x * 0.9)\n",
    "\n",
    "# Using np.where (vectorized if-else)\n",
    "df['Quality'] = np.where(df['Rating'] >= 4.5, 'Excellent', 'Good')\n",
    "\n",
    "print(df[['Product', 'Price', 'Price_Category', 'Quality']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by single column\n",
    "sorted_price = df.sort_values('Price', ascending=False)\n",
    "print(\"Top 5 by price:\")\n",
    "print(sorted_price[['Product', 'Price']].head())\n",
    "\n",
    "# Sort by multiple columns\n",
    "sorted_multi = df.sort_values(['Product', 'Price'], ascending=[True, False])\n",
    "print(\"\\nSorted by Product (asc), then Price (desc):\")\n",
    "print(sorted_multi[['Product', 'Price']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with missing values\n",
    "df_missing = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [np.nan, 2, 3, np.nan, 5],\n",
    "    'C': [1, 2, 3, 4, 5]\n",
    "})\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(df_missing)\n",
    "\n",
    "# Detect missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df_missing.isnull().sum())\n",
    "\n",
    "# Drop rows with any missing values\n",
    "print(\"\\nAfter dropna():\")\n",
    "print(df_missing.dropna())\n",
    "\n",
    "# Fill missing values\n",
    "print(\"\\nFill with 0:\")\n",
    "print(df_missing.fillna(0))\n",
    "\n",
    "# Fill with mean\n",
    "df_filled = df_missing.copy()\n",
    "df_filled['A'].fillna(df_filled['A'].mean(), inplace=True)\n",
    "print(\"\\nFill column A with mean:\")\n",
    "print(df_filled)\n",
    "\n",
    "# Forward fill (propagate last valid value)\n",
    "print(\"\\nForward fill:\")\n",
    "print(df_missing.fillna(method='ffill'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Grouping and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by single column\n",
    "product_stats = df.groupby('Product')['Revenue'].sum().sort_values(ascending=False)\n",
    "print(\"Total Revenue by Product:\")\n",
    "print(product_stats)\n",
    "\n",
    "# Multiple aggregations\n",
    "summary = df.groupby('Product').agg({\n",
    "    'Price': ['mean', 'min', 'max'],\n",
    "    'Quantity': 'sum',\n",
    "    'Rating': 'mean'\n",
    "})\n",
    "print(\"\\nDetailed summary:\")\n",
    "print(summary)\n",
    "\n",
    "# Group by multiple columns\n",
    "multi_group = df.groupby(['Product', 'Price_Category']).size()\n",
    "print(\"\\nCount by Product and Price Category:\")\n",
    "print(multi_group)\n",
    "\n",
    "# Custom aggregation\n",
    "def price_range(x):\n",
    "    return x.max() - x.min()\n",
    "\n",
    "price_ranges = df.groupby('Product')['Price'].agg(price_range)\n",
    "print(\"\\nPrice Range by Product:\")\n",
    "print(price_ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Merging and Joining DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataframes\n",
    "employees = pd.DataFrame({\n",
    "    'emp_id': [1, 2, 3, 4],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
    "    'dept_id': [10, 20, 10, 30]\n",
    "})\n",
    "\n",
    "departments = pd.DataFrame({\n",
    "    'dept_id': [10, 20, 30],\n",
    "    'dept_name': ['Engineering', 'Marketing', 'Sales']\n",
    "})\n",
    "\n",
    "# Inner join (default)\n",
    "inner = pd.merge(employees, departments, on='dept_id')\n",
    "print(\"Inner join:\")\n",
    "print(inner)\n",
    "\n",
    "# Left join\n",
    "left = pd.merge(employees, departments, on='dept_id', how='left')\n",
    "print(\"\\nLeft join:\")\n",
    "print(left)\n",
    "\n",
    "# Concatenate vertically\n",
    "df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})\n",
    "combined = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"\\nConcatenated:\")\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Pivot Tables and Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sales data\n",
    "sales = pd.DataFrame({\n",
    "    'Date': pd.date_range('2024-01-01', periods=12, freq='M'),\n",
    "    'Product': ['A', 'B'] * 6,\n",
    "    'Region': ['North', 'South', 'North', 'South'] * 3,\n",
    "    'Sales': np.random.randint(100, 1000, 12)\n",
    "})\n",
    "\n",
    "print(\"Sales data:\")\n",
    "print(sales.head())\n",
    "\n",
    "# Pivot table\n",
    "pivot = pd.pivot_table(sales, \n",
    "                       values='Sales',\n",
    "                       index='Product',\n",
    "                       columns='Region',\n",
    "                       aggfunc='sum')\n",
    "print(\"\\nPivot table:\")\n",
    "print(pivot)\n",
    "\n",
    "# Melt (unpivot)\n",
    "melted = pivot.reset_index().melt(id_vars='Product', \n",
    "                                   var_name='Region',\n",
    "                                   value_name='Sales')\n",
    "print(\"\\nMelted back:\")\n",
    "print(melted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Time Series Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series data\n",
    "dates = pd.date_range('2024-01-01', periods=100, freq='D')\n",
    "ts = pd.Series(np.random.randn(100).cumsum(), index=dates)\n",
    "\n",
    "print(\"Time series (first 5):\")\n",
    "print(ts.head())\n",
    "\n",
    "# Date-based selection\n",
    "print(\"\\nJanuary data:\")\n",
    "print(ts['2024-01'].head())\n",
    "\n",
    "# Resampling\n",
    "weekly = ts.resample('W').mean()\n",
    "print(\"\\nWeekly average (first 3):\")\n",
    "print(weekly.head(3))\n",
    "\n",
    "# Rolling window\n",
    "rolling_mean = ts.rolling(window=7).mean()\n",
    "print(\"\\n7-day rolling mean (first 10):\")\n",
    "print(rolling_mean.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Data Visualization with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Bar chart\n",
    "product_stats.plot(kind='bar', figsize=(10, 6), color='skyblue')\n",
    "plt.title('Total Revenue by Product')\n",
    "plt.ylabel('Revenue ($)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Histogram\n",
    "df['Price'].hist(bins=20, figsize=(10, 6), edgecolor='black')\n",
    "plt.title('Price Distribution')\n",
    "plt.xlabel('Price ($)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Box plot\n",
    "df.boxplot(column='Price', by='Product', figsize=(10, 6))\n",
    "plt.title('Price Distribution by Product')\n",
    "plt.suptitle('')  # Remove default title\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot\n",
    "df.plot.scatter(x='Price', y='Rating', figsize=(10, 6), alpha=0.5)\n",
    "plt.title('Price vs Rating')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Real-World Example: Sales Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create realistic sales dataset\n",
    "np.random.seed(42)\n",
    "n_records = 500\n",
    "\n",
    "sales_df = pd.DataFrame({\n",
    "    'Date': pd.date_range('2024-01-01', periods=n_records, freq='H'),\n",
    "    'Product': np.random.choice(['Laptop', 'Phone', 'Tablet', 'Monitor'], n_records),\n",
    "    'Region': np.random.choice(['North', 'South', 'East', 'West'], n_records),\n",
    "    'Salesperson': np.random.choice(['Alice', 'Bob', 'Charlie', 'Diana'], n_records),\n",
    "    'Quantity': np.random.randint(1, 5, n_records),\n",
    "    'Unit_Price': np.random.randint(200, 1500, n_records)\n",
    "})\n",
    "\n",
    "# Calculate revenue\n",
    "sales_df['Revenue'] = sales_df['Quantity'] * sales_df['Unit_Price']\n",
    "\n",
    "print(\"Sales Dataset:\")\n",
    "print(sales_df.head())\n",
    "\n",
    "# Analysis 1: Top products by revenue\n",
    "top_products = sales_df.groupby('Product')['Revenue'].sum().sort_values(ascending=False)\n",
    "print(\"\\nTop Products by Revenue:\")\n",
    "print(top_products)\n",
    "\n",
    "# Analysis 2: Best performing regions\n",
    "region_performance = sales_df.groupby('Region').agg({\n",
    "    'Revenue': 'sum',\n",
    "    'Quantity': 'sum',\n",
    "    'Salesperson': 'count'\n",
    "}).rename(columns={'Salesperson': 'Transactions'})\n",
    "print(\"\\nRegion Performance:\")\n",
    "print(region_performance)\n",
    "\n",
    "# Analysis 3: Top salespeople\n",
    "salesperson_stats = sales_df.groupby('Salesperson').agg({\n",
    "    'Revenue': ['sum', 'mean'],\n",
    "    'Quantity': 'sum'\n",
    "}).round(2)\n",
    "print(\"\\nSalesperson Performance:\")\n",
    "print(salesperson_stats)\n",
    "\n",
    "# Analysis 4: Daily trends\n",
    "sales_df['Date_Only'] = sales_df['Date'].dt.date\n",
    "daily_revenue = sales_df.groupby('Date_Only')['Revenue'].sum()\n",
    "print(\"\\nDaily Revenue (first 5 days):\")\n",
    "print(daily_revenue.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Student Performance Analysis\n",
    "\n",
    "Create and analyze a student grades dataset:\n",
    "1. Create DataFrame with students, subjects, and scores\n",
    "2. Calculate average score per student\n",
    "3. Find top 3 students\n",
    "4. Calculate average score per subject\n",
    "5. Identify students scoring below 70 in any subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: E-commerce Data Cleaning\n",
    "\n",
    "Given messy e-commerce data:\n",
    "1. Handle missing values appropriately\n",
    "2. Remove duplicates\n",
    "3. Convert data types correctly\n",
    "4. Create new calculated columns\n",
    "5. Export cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Sales Dashboard\n",
    "\n",
    "Create a mini sales dashboard:\n",
    "1. Load sales data\n",
    "2. Calculate key metrics (total revenue, avg order value, etc.)\n",
    "3. Create visualizations (revenue trend, top products, regional breakdown)\n",
    "4. Generate summary report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "âœ… **DataFrames** are 2D labeled data structures (like Excel spreadsheets)\n",
    "\n",
    "âœ… **Read/write** data in many formats (CSV, Excel, JSON, SQL)\n",
    "\n",
    "âœ… **loc/iloc** for label-based and integer-based indexing\n",
    "\n",
    "âœ… **Boolean indexing** for powerful filtering\n",
    "\n",
    "âœ… **GroupBy** for aggregation and summary statistics\n",
    "\n",
    "âœ… **Merge/join** to combine datasets\n",
    "\n",
    "âœ… **Handle missing data** with dropna(), fillna()\n",
    "\n",
    "âœ… **Vectorized operations** are fast - avoid loops!\n",
    "\n",
    "âœ… **Built-in plotting** for quick visualizations\n",
    "\n",
    "âœ… **Method chaining** creates readable data pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pro Tips\n",
    "\n",
    "ðŸ’¡ **Use inplace=False** - Safer to assign results to new variable\n",
    "\n",
    "ðŸ’¡ **Check dtypes early** - Wrong types cause unexpected behavior\n",
    "\n",
    "ðŸ’¡ **copy() when needed** - Avoid unintended modifications\n",
    "\n",
    "ðŸ’¡ **Use query()** for complex filters: `df.query('Price > 100 & Rating >= 4')`\n",
    "\n",
    "ðŸ’¡ **Vectorize operations** - Much faster than loops\n",
    "\n",
    "ðŸ’¡ **Use categorical dtype** for repeated string values (saves memory)\n",
    "\n",
    "ðŸ’¡ **Profile memory usage** - `df.memory_usage(deep=True)`\n",
    "\n",
    "ðŸ’¡ **Read in chunks** for large files: `pd.read_csv('big.csv', chunksize=10000)`\n",
    "\n",
    "ðŸ’¡ **Use .loc for setting values** - Avoids SettingWithCopyWarning\n",
    "\n",
    "ðŸ’¡ **explore() method** - Interactive pandas profiling (if available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Mistakes to Avoid\n",
    "\n",
    "âŒ **Chained indexing** - `df[df.A > 0][df.B < 5] = value`\n",
    "âœ… Use Boolean mask: `df.loc[(df.A > 0) & (df.B < 5)] = value`\n",
    "\n",
    "âŒ **Iterating with loops** - Very slow\n",
    "âœ… Use vectorized operations or apply()\n",
    "\n",
    "âŒ **Modifying while iterating** - Causes errors\n",
    "âœ… Create new DataFrame or use copy()\n",
    "\n",
    "âŒ **Not handling missing data** - NaN breaks calculations\n",
    "âœ… Use dropna(), fillna(), or handle explicitly\n",
    "\n",
    "âŒ **Forgetting axis parameter** - axis=0 (rows), axis=1 (columns)\n",
    "âœ… Always check which axis you're operating on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "You now have solid Pandas fundamentals! Next topics:\n",
    "\n",
    "1. **Advanced Pandas** - MultiIndex, window functions, performance optimization\n",
    "2. **Data Visualization** - Seaborn, Plotly for advanced plots\n",
    "3. **SQL with Pandas** - Read from databases, write queries\n",
    "4. **Big Data** - Dask, PySpark for datasets larger than RAM\n",
    "5. **Data Science Projects** - End-to-end analysis workflows\n",
    "\n",
    "**Practice Projects:**\n",
    "- Analyze COVID-19 data\n",
    "- Build a stock market analyzer\n",
    "- Create a customer segmentation analysis\n",
    "- Analyze your own data (finance, health, etc.)\n",
    "\n",
    "**Resources:**\n",
    "- Pandas documentation: https://pandas.pydata.org/docs/\n",
    "- Kaggle datasets: https://kaggle.com/datasets\n",
    "- 10 Minutes to Pandas: https://pandas.pydata.org/docs/user_guide/10min.html\n",
    "\n",
    "Pandas is the foundation of data analysis in Python - master it! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
