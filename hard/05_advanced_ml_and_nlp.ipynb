{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5: Advanced ML and Natural Language Processing\n",
    "\n",
    "Explore advanced machine learning topics including NLP, ensemble methods, and model optimization.\n",
    "\n",
    "## What You'll Learn\n",
    "- Ensemble learning methods\n",
    "- Natural Language Processing (NLP) basics\n",
    "- Text classification and sentiment analysis\n",
    "- Word embeddings and transformers\n",
    "- Hyperparameter tuning\n",
    "- Model deployment considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "\n",
    "Combining multiple models for better predictions:\n",
    "- **Bagging**: Train models on random subsets (Random Forest)\n",
    "- **Boosting**: Sequential learning from mistakes (XGBoost, AdaBoost)\n",
    "- **Stacking**: Combine predictions with meta-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn xgboost\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Generate dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, \n",
    "                          n_redundant=5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Dataset: {X_train.shape[0]} training, {X_test.shape[0]} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest (Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "print(f\"Random Forest Accuracy: {rf_acc * 100:.2f}%\")\n",
    "\n",
    "# Feature importance\n",
    "print(\"\\nTop 5 Important Features:\")\n",
    "feature_importance = sorted(zip(range(20), rf_model.feature_importances_), \n",
    "                           key=lambda x: x[1], reverse=True)[:5]\n",
    "for feat, importance in feature_importance:\n",
    "    print(f\"  Feature {feat}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, \n",
    "                                     max_depth=5, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "gb_pred = gb_model.predict(X_test)\n",
    "gb_acc = accuracy_score(y_test, gb_pred)\n",
    "print(f\"Gradient Boosting Accuracy: {gb_acc * 100:.2f}%\")\n",
    "\n",
    "# XGBoost (optimized gradient boosting)\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, \n",
    "                             max_depth=5, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "xgb_acc = accuracy_score(y_test, xgb_pred)\n",
    "print(f\"XGBoost Accuracy: {xgb_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine multiple models\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=50, random_state=42)),\n",
    "        ('lr', LogisticRegression(random_state=42))\n",
    "    ],\n",
    "    voting='soft'  # Use probability averages\n",
    ")\n",
    "\n",
    "voting_model.fit(X_train, y_train)\n",
    "voting_pred = voting_model.predict(X_test)\n",
    "voting_acc = accuracy_score(y_test, voting_pred)\n",
    "\n",
    "print(f\"\\nVoting Ensemble Accuracy: {voting_acc * 100:.2f}%\")\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"  Random Forest: {rf_acc * 100:.2f}%\")\n",
    "print(f\"  Gradient Boosting: {gb_acc * 100:.2f}%\")\n",
    "print(f\"  XGBoost: {xgb_acc * 100:.2f}%\")\n",
    "print(f\"  Voting Ensemble: {voting_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "### Text Preprocessing and Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "\n",
    "# Sample text data\n",
    "texts = [\n",
    "    \"This movie was absolutely amazing! Best film I've ever seen.\",\n",
    "    \"Terrible movie, waste of time and money. Very disappointing.\",\n",
    "    \"Great acting and plot. Highly recommend this movie!\",\n",
    "    \"Boring and predictable. Would not watch again.\",\n",
    "    \"Excellent cinematography and soundtrack. A masterpiece!\",\n",
    "    \"Worst movie ever. Don't waste your time.\"\n",
    "]\n",
    "\n",
    "labels = [1, 0, 1, 0, 1, 0]  # 1=positive, 0=negative\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Preprocess\n",
    "processed_texts = [preprocess_text(text) for text in texts]\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=100)\n",
    "X_tfidf = tfidf.fit_transform(processed_texts)\n",
    "\n",
    "print(\"TF-IDF Matrix shape:\", X_tfidf.shape)\n",
    "print(\"\\nTop features:\", tfidf.get_feature_names_out()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create larger dataset for sentiment analysis\n",
    "positive_reviews = [\n",
    "    \"Excellent product, highly recommended!\",\n",
    "    \"Love it! Best purchase ever.\",\n",
    "    \"Amazing quality and fast delivery.\",\n",
    "    \"Perfect! Exactly what I needed.\",\n",
    "    \"Outstanding service and product.\"\n",
    "]\n",
    "\n",
    "negative_reviews = [\n",
    "    \"Terrible quality, very disappointed.\",\n",
    "    \"Waste of money, do not buy.\",\n",
    "    \"Poor customer service and defective product.\",\n",
    "    \"Not as described, requesting refund.\",\n",
    "    \"Awful experience, would give zero stars.\"\n",
    "]\n",
    "\n",
    "# Combine and label\n",
    "all_reviews = positive_reviews + negative_reviews\n",
    "sentiment_labels = [1] * len(positive_reviews) + [0] * len(negative_reviews)\n",
    "\n",
    "# Create pipeline\n",
    "sentiment_pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Train\n",
    "sentiment_pipeline.fit(all_reviews, sentiment_labels)\n",
    "\n",
    "# Test on new reviews\n",
    "test_reviews = [\n",
    "    \"This is absolutely fantastic!\",\n",
    "    \"Very poor quality and service\",\n",
    "    \"Good value for money\"\n",
    "]\n",
    "\n",
    "predictions = sentiment_pipeline.predict(test_reviews)\n",
    "probabilities = sentiment_pipeline.predict_proba(test_reviews)\n",
    "\n",
    "print(\"Sentiment Predictions:\")\n",
    "for review, pred, prob in zip(test_reviews, predictions, probabilities):\n",
    "    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    confidence = max(prob) * 100\n",
    "    print(f\"'{review}'\")\n",
    "    print(f\"  -> {sentiment} ({confidence:.1f}% confident)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "\n",
    "# Prepare text data\n",
    "tokenizer = Tokenizer(num_words=1000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(all_reviews)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(all_reviews)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=20, padding='post')\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(f\"Padded sequence shape: {padded_sequences.shape}\")\n",
    "\n",
    "# Build LSTM model with embeddings\n",
    "embedding_dim = 16\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    Embedding(1000, embedding_dim, input_length=20),\n",
    "    LSTM(32, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(16),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "rf_grid = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(rf_grid, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "print(\"Running grid search... (this may take a moment)\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_ * 100:.2f}%\")\n",
    "\n",
    "# Test best model\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(f\"Test set accuracy: {test_score * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Build an advanced text classification system:\n",
    "1. Collect or create a dataset of news articles with categories (politics, sports, technology, etc.)\n",
    "2. Implement advanced text preprocessing (stemming, lemmatization, stop word removal)\n",
    "3. Compare different vectorization methods (Bag of Words, TF-IDF, Word2Vec)\n",
    "4. Build and compare multiple models (Naive Bayes, Random Forest, LSTM)\n",
    "5. Use hyperparameter tuning to optimize the best model\n",
    "6. Implement k-fold cross-validation\n",
    "7. Create a confusion matrix and ROC curves\n",
    "8. Build a simple deployment function that takes raw text and returns prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
