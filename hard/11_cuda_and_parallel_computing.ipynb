{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Level: CUDA & GPU Parallel Computing\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "**The Problem**: Modern CPUs have 4-16 cores. Modern GPUs have thousands of cores. For the right workloads, GPUs can be 10-100x faster than CPUs.\n",
    "\n",
    "**Where GPUs Dominate**:\n",
    "- **Deep Learning**: Training neural networks (PyTorch, TensorFlow)\n",
    "- **Scientific Computing**: Physics simulations, climate modeling\n",
    "- **Image/Video Processing**: Real-time rendering, computer vision\n",
    "- **Cryptography**: Password cracking, blockchain mining\n",
    "- **Financial Modeling**: Monte Carlo simulations, risk analysis\n",
    "- **Bioinformatics**: Gene sequencing, protein folding\n",
    "\n",
    "**Why This Matters**:\n",
    "- **Speed**: Train models in hours instead of weeks\n",
    "- **Scale**: Process billions of data points in real-time\n",
    "- **Cost**: One GPU can replace dozens of CPU cores\n",
    "- **Energy**: Higher performance per watt\n",
    "\n",
    "**What You'll Learn**:\n",
    "- GPU architecture and CUDA programming model\n",
    "- PyCUDA for Python-CUDA integration\n",
    "- CuPy - NumPy for GPUs\n",
    "- Parallel algorithms and patterns\n",
    "- GPU memory management\n",
    "- Multi-GPU programming\n",
    "- Real-world optimization techniques\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: GPU Architecture Fundamentals\n",
    "\n",
    "### CPU vs GPU: Different Design Philosophy\n",
    "\n",
    "**CPU (Central Processing Unit)**:\n",
    "- **Few powerful cores** (4-16)\n",
    "- **High clock speed** (3-5 GHz)\n",
    "- **Large cache** (MB of L1/L2/L3)\n",
    "- **Low latency**: Optimized for sequential tasks\n",
    "- **Complex control logic**: Branch prediction, out-of-order execution\n",
    "\n",
    "**GPU (Graphics Processing Unit)**:\n",
    "- **Thousands of simple cores** (2,000-10,000+)\n",
    "- **Lower clock speed** (1-2 GHz)\n",
    "- **Small cache per core**: Focus on throughput\n",
    "- **High throughput**: Optimized for parallel tasks\n",
    "- **Simple control**: SIMT (Single Instruction, Multiple Threads)\n",
    "\n",
    "### NVIDIA GPU Architecture\n",
    "\n",
    "```\n",
    "GPU\n",
    "â”‚\n",
    "â”œâ”€ Streaming Multiprocessor (SM) Ã— 80-100+\n",
    "â”‚  â”‚\n",
    "â”‚  â”œâ”€ CUDA Cores Ã— 64-128 per SM\n",
    "â”‚  â”œâ”€ Tensor Cores (for AI)\n",
    "â”‚  â”œâ”€ Shared Memory (fast, 48-96 KB)\n",
    "â”‚  â”œâ”€ L1 Cache\n",
    "â”‚  â””â”€ Registers\n",
    "â”‚\n",
    "â”œâ”€ L2 Cache (shared, several MB)\n",
    "â”‚\n",
    "â””â”€ Global Memory (VRAM, 8-80 GB)\n",
    "   - High bandwidth (1000+ GB/s)\n",
    "   - High latency (100s of cycles)\n",
    "```\n",
    "\n",
    "### CUDA Programming Model\n",
    "\n",
    "**Key Concepts**:\n",
    "1. **Kernel**: Function that runs on GPU\n",
    "2. **Thread**: Smallest execution unit\n",
    "3. **Block**: Group of threads (up to 1024)\n",
    "4. **Grid**: Collection of blocks\n",
    "\n",
    "```\n",
    "Grid\n",
    "â”œâ”€ Block(0,0)     Block(1,0)     Block(2,0)\n",
    "â”‚  â”œâ”€ Thread(0,0) â”œâ”€ Thread(0,0) â”œâ”€ Thread(0,0)\n",
    "â”‚  â”œâ”€ Thread(1,0) â”œâ”€ Thread(1,0) â”œâ”€ Thread(1,0)\n",
    "â”‚  â”œâ”€ Thread(2,0) â”œâ”€ Thread(2,0) â”œâ”€ Thread(2,0)\n",
    "â”‚  â””â”€ ...         â””â”€ ...         â””â”€ ...\n",
    "â”‚\n",
    "â”œâ”€ Block(0,1)     Block(1,1)     Block(2,1)\n",
    "   â””â”€ ...         â””â”€ ...         â””â”€ ...\n",
    "```\n",
    "\n",
    "**Memory Hierarchy** (fast to slow):\n",
    "1. **Registers**: Per-thread, fastest (1 cycle)\n",
    "2. **Shared Memory**: Per-block, very fast (1-2 cycles)\n",
    "3. **L1/L2 Cache**: Automatic, fast\n",
    "4. **Global Memory**: Slowest (100s of cycles) but largest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Checking GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def check_cuda():\n",
    "    \"\"\"Check CUDA and GPU availability.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CUDA & GPU Availability Check\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check nvidia-smi\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"\\nâœ“ NVIDIA GPU detected!\\n\")\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(\"\\nâœ— nvidia-smi not available\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"\\nâœ— NVIDIA drivers not installed\")\n",
    "    \n",
    "    # Check PyTorch CUDA\n",
    "    try:\n",
    "        import torch\n",
    "        cuda_available = torch.cuda.is_available()\n",
    "        print(f\"\\nPyTorch CUDA available: {cuda_available}\")\n",
    "        if cuda_available:\n",
    "            print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "            print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    except ImportError:\n",
    "        print(\"\\nPyTorch not installed. Install with: pip install torch\")\n",
    "    \n",
    "    # Check CuPy\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        print(f\"\\nCuPy available: True\")\n",
    "        print(f\"CuPy CUDA version: {cp.cuda.runtime.runtimeGetVersion()}\")\n",
    "    except ImportError:\n",
    "        print(\"\\nCuPy not installed. Install with: pip install cupy-cuda11x\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nCuPy error: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "check_cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If you don't have a GPU, many cloud platforms offer GPU access:\n",
    "- **Google Colab**: Free T4 GPU (15GB VRAM)\n",
    "- **Kaggle**: Free P100 GPU (30 hours/week)\n",
    "- **AWS/GCP/Azure**: Pay-per-use GPU instances\n",
    "- **Lambda Labs**: Specialized GPU cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: CuPy - NumPy for GPUs\n",
    "\n",
    "CuPy is a NumPy-compatible library that runs on NVIDIA GPUs. It's the easiest way to start GPU computing in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CuPy basics (pseudocode if GPU not available)\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "    GPU_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"CuPy not available. Showing pseudocode examples.\")\n",
    "    GPU_AVAILABLE = False\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    # Example 1: Array creation and operations\n",
    "    print(\"Example 1: Basic Operations\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # CPU (NumPy)\n",
    "    x_cpu = np.array([1, 2, 3, 4, 5])\n",
    "    y_cpu = np.array([6, 7, 8, 9, 10])\n",
    "    z_cpu = x_cpu + y_cpu\n",
    "    print(f\"NumPy result: {z_cpu}\")\n",
    "    \n",
    "    # GPU (CuPy) - Same syntax!\n",
    "    x_gpu = cp.array([1, 2, 3, 4, 5])\n",
    "    y_gpu = cp.array([6, 7, 8, 9, 10])\n",
    "    z_gpu = x_gpu + y_gpu\n",
    "    print(f\"CuPy result: {z_gpu}\")\n",
    "    \n",
    "    # Transfer between CPU and GPU\n",
    "    cpu_array = cp.asnumpy(z_gpu)  # GPU â†’ CPU\n",
    "    gpu_array = cp.asarray(z_cpu)  # CPU â†’ GPU\n",
    "    \n",
    "    print(\"\\nExample 2: Performance Comparison\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Large matrix operations\n",
    "    size = 10000\n",
    "    \n",
    "    # CPU\n",
    "    a_cpu = np.random.rand(size, size).astype(np.float32)\n",
    "    b_cpu = np.random.rand(size, size).astype(np.float32)\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    c_cpu = np.dot(a_cpu, b_cpu)\n",
    "    time_cpu = time.perf_counter() - start\n",
    "    \n",
    "    # GPU\n",
    "    a_gpu = cp.random.rand(size, size, dtype=cp.float32)\n",
    "    b_gpu = cp.random.rand(size, size, dtype=cp.float32)\n",
    "    \n",
    "    # Warm up\n",
    "    _ = cp.dot(a_gpu, b_gpu)\n",
    "    cp.cuda.Stream.null.synchronize()  # Wait for GPU\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    c_gpu = cp.dot(a_gpu, b_gpu)\n",
    "    cp.cuda.Stream.null.synchronize()  # Important: wait for GPU to finish!\n",
    "    time_gpu = time.perf_counter() - start\n",
    "    \n",
    "    print(f\"Matrix multiplication ({size}Ã—{size}):\")\n",
    "    print(f\"NumPy (CPU): {time_cpu:.4f}s\")\n",
    "    print(f\"CuPy (GPU):  {time_gpu:.4f}s\")\n",
    "    print(f\"Speedup: {time_cpu/time_gpu:.1f}x faster!\")\n",
    "    \n",
    "else:\n",
    "    print(\"\"\"\n",
    "    CuPy Example (Pseudocode):\n",
    "    \n",
    "    import cupy as cp\n",
    "    \n",
    "    # Create arrays on GPU\n",
    "    x_gpu = cp.array([1, 2, 3, 4, 5])\n",
    "    y_gpu = cp.array([6, 7, 8, 9, 10])\n",
    "    \n",
    "    # Operations run on GPU automatically\n",
    "    z_gpu = x_gpu + y_gpu\n",
    "    \n",
    "    # Transfer data: GPU â†” CPU\n",
    "    cpu_array = cp.asnumpy(z_gpu)  # GPU â†’ CPU\n",
    "    gpu_array = cp.asarray(cpu_array)  # CPU â†’ GPU\n",
    "    \n",
    "    # All NumPy operations work!\n",
    "    result = cp.mean(x_gpu)\n",
    "    \n",
    "    Speedup: Typically 10-100x for large arrays\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CuPy Best Practices\n",
    "\n",
    "1. **Minimize CPU â†” GPU transfers**: Keep data on GPU\n",
    "2. **Use synchronize()**: GPU operations are async\n",
    "3. **Batch operations**: Single large operation > many small ones\n",
    "4. **Use float32**: Twice as fast as float64 on most GPUs\n",
    "5. **Reuse arrays**: Avoid frequent allocation/deallocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced CuPy: Custom kernels\n",
    "if GPU_AVAILABLE:\n",
    "    # Element-wise kernel (like NumPy ufunc)\n",
    "    from cupy import ElementwiseKernel\n",
    "    \n",
    "    # Kernel definition (C++ syntax)\n",
    "    add_kernel = ElementwiseKernel(\n",
    "        'float32 x, float32 y',  # Input types\n",
    "        'float32 z',  # Output type\n",
    "        'z = x + y',  # Operation\n",
    "        'add_kernel'  # Name\n",
    "    )\n",
    "    \n",
    "    # Use it\n",
    "    x = cp.arange(1000000, dtype=cp.float32)\n",
    "    y = cp.arange(1000000, dtype=cp.float32)\n",
    "    z = add_kernel(x, y)\n",
    "    \n",
    "    print(f\"Custom kernel result: {z[:5]}...\")\n",
    "    \n",
    "    # More complex: squared difference\n",
    "    squared_diff_kernel = ElementwiseKernel(\n",
    "        'float32 x, float32 y',\n",
    "        'float32 z',\n",
    "        'z = (x - y) * (x - y)',\n",
    "        'squared_diff'\n",
    "    )\n",
    "    \n",
    "    result = squared_diff_kernel(x, y)\n",
    "    print(f\"Squared difference: {result[:5]}...\")\n",
    "else:\n",
    "    print(\"Custom CuPy kernels allow writing GPU code in C++ syntax!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: PyTorch GPU Acceleration\n",
    "\n",
    "PyTorch provides the easiest path to GPU computing for deep learning and scientific computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"PyTorch not installed. Install with: pip install torch\")\n",
    "    TORCH_AVAILABLE = False\n",
    "\n",
    "if TORCH_AVAILABLE:\n",
    "    print(\"PyTorch GPU Example\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Check device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "    # Create tensors on GPU\n",
    "    size = 5000\n",
    "    \n",
    "    # Method 1: Create on GPU directly\n",
    "    x_gpu = torch.rand(size, size, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    y_gpu = torch.rand(size, size, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Method 2: Create on CPU then move\n",
    "    x_cpu = torch.rand(size, size)\n",
    "    if torch.cuda.is_available():\n",
    "        x_gpu = x_cpu.to('cuda')  # or .cuda()\n",
    "    \n",
    "    # Benchmark\n",
    "    if torch.cuda.is_available():\n",
    "        # GPU\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        start.record()\n",
    "        z_gpu = torch.mm(x_gpu, y_gpu)  # Matrix multiply\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        time_gpu = start.elapsed_time(end) / 1000  # ms to seconds\n",
    "        \n",
    "        # CPU\n",
    "        x_cpu = torch.rand(size, size)\n",
    "        y_cpu = torch.rand(size, size)\n",
    "        \n",
    "        start_cpu = time.perf_counter()\n",
    "        z_cpu = torch.mm(x_cpu, y_cpu)\n",
    "        time_cpu = time.perf_counter() - start_cpu\n",
    "        \n",
    "        print(f\"\\nMatrix multiplication ({size}Ã—{size}):\")\n",
    "        print(f\"CPU: {time_cpu:.4f}s\")\n",
    "        print(f\"GPU: {time_gpu:.4f}s\")\n",
    "        print(f\"Speedup: {time_cpu/time_gpu:.1f}x faster!\")\n",
    "    else:\n",
    "        print(\"\\nNo GPU available for benchmarking\")\n",
    "        \n",
    "else:\n",
    "    print(\"\"\"\n",
    "    PyTorch GPU Example (Pseudocode):\n",
    "    \n",
    "    import torch\n",
    "    \n",
    "    # Check GPU availability\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Create tensor on GPU\n",
    "    x = torch.rand(1000, 1000, device='cuda')\n",
    "    y = torch.rand(1000, 1000, device='cuda')\n",
    "    \n",
    "    # All operations run on GPU\n",
    "    z = torch.mm(x, y)\n",
    "    \n",
    "    # Move between devices\n",
    "    x_cpu = x.cpu()  # GPU â†’ CPU\n",
    "    x_gpu = x_cpu.cuda()  # CPU â†’ GPU\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Parallel Algorithm Patterns\n",
    "\n",
    "Certain algorithms are naturally parallel and map perfectly to GPUs.\n",
    "\n",
    "### Pattern 1: Map (Element-wise Operations)\n",
    "\n",
    "Apply same operation to each element independently.\n",
    "\n",
    "**Examples**: Array addition, sigmoid activation, image filters\n",
    "\n",
    "```python\n",
    "# CPU: Sequential\n",
    "for i in range(n):\n",
    "    output[i] = func(input[i])\n",
    "\n",
    "# GPU: Parallel (each thread handles one element)\n",
    "thread_id = blockIdx.x * blockDim.x + threadIdx.x\n",
    "if thread_id < n:\n",
    "    output[thread_id] = func(input[thread_id])\n",
    "```\n",
    "\n",
    "### Pattern 2: Reduce (Aggregation)\n",
    "\n",
    "Combine all elements into single value.\n",
    "\n",
    "**Examples**: Sum, max, min, mean\n",
    "\n",
    "```\n",
    "Tree-based reduction:\n",
    "[1, 2, 3, 4, 5, 6, 7, 8]\n",
    " â””â”€â”¬â”€â”˜ â””â”€â”¬â”€â”˜ â””â”€â”¬â”€â”˜ â””â”€â”¬â”€â”˜   Step 1: Pair-wise\n",
    "   3     7     11    15\n",
    "   â””â”€â”€â”¬â”€â”€â”˜     â””â”€â”€â”¬â”€â”€â”˜      Step 2: Pair-wise\n",
    "      10          26\n",
    "      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          Step 3: Final\n",
    "            36\n",
    "```\n",
    "\n",
    "### Pattern 3: Scan (Prefix Sum)\n",
    "\n",
    "Compute running aggregation.\n",
    "\n",
    "**Examples**: Cumulative sum, histogram, sorting\n",
    "\n",
    "```\n",
    "Input:  [1, 2, 3, 4, 5]\n",
    "Output: [1, 3, 6, 10, 15]  (cumulative sum)\n",
    "```\n",
    "\n",
    "### Pattern 4: Stencil (Neighbor Operations)\n",
    "\n",
    "Compute based on neighbors in structured grid.\n",
    "\n",
    "**Examples**: Convolution, blur, diffusion\n",
    "\n",
    "```\n",
    "3Ã—3 kernel:\n",
    "  â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”\n",
    "  â”‚ 1 â”‚ 2 â”‚ 1 â”‚\n",
    "  â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤\n",
    "  â”‚ 2 â”‚ 4 â”‚ 2 â”‚  Apply to each pixel\n",
    "  â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤\n",
    "  â”‚ 1 â”‚ 2 â”‚ 1 â”‚\n",
    "  â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: GPU Memory Management\n",
    "\n",
    "Efficient memory usage is crucial for GPU performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE and torch.cuda.is_available():\n",
    "    print(\"GPU Memory Management\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Memory stats\n",
    "    def print_gpu_memory():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"Reserved:  {reserved:.2f} GB\")\n",
    "        print(f\"Total:     {total:.2f} GB\")\n",
    "    \n",
    "    print(\"\\nInitial state:\")\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    # Allocate memory\n",
    "    print(\"\\nAfter creating 5000Ã—5000 tensor:\")\n",
    "    x = torch.rand(5000, 5000, device='cuda')\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    # Free memory\n",
    "    del x\n",
    "    torch.cuda.empty_cache()  # Release reserved memory\n",
    "    \n",
    "    print(\"\\nAfter deleting tensor and clearing cache:\")\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    # Memory-efficient operations\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"Memory-Efficient Patterns:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # In-place operations save memory\n",
    "    x = torch.rand(1000, 1000, device='cuda')\n",
    "    \n",
    "    # Bad: Creates new tensor\n",
    "    # y = x + 1\n",
    "    \n",
    "    # Good: In-place (appends underscore)\n",
    "    x.add_(1)  # Modifies x directly\n",
    "    \n",
    "    # Context manager for automatic cleanup\n",
    "    with torch.cuda.device(0):\n",
    "        temp = torch.rand(1000, 1000, device='cuda')\n",
    "        # Automatically freed when exiting context\n",
    "    \n",
    "    print(\"\\nMemory Best Practices:\")\n",
    "    print(\"1. Use in-place operations: tensor.add_() vs tensor + 1\")\n",
    "    print(\"2. Delete large tensors when done: del tensor\")\n",
    "    print(\"3. Clear cache periodically: torch.cuda.empty_cache()\")\n",
    "    print(\"4. Use mixed precision (float16): Halves memory usage\")\n",
    "    print(\"5. Batch processing: Process data in chunks\")\n",
    "    print(\"6. Gradient checkpointing: Trade compute for memory\")\n",
    "    \n",
    "else:\n",
    "    print(\"GPU Memory Management (Conceptual):\")\n",
    "    print(\"\"\"\n",
    "    GPU memory is limited (8-80 GB typical).\n",
    "    \n",
    "    Best Practices:\n",
    "    1. Monitor: torch.cuda.memory_allocated()\n",
    "    2. Free: del tensor, torch.cuda.empty_cache()\n",
    "    3. In-place ops: tensor.add_(1) instead of tensor + 1\n",
    "    4. Mixed precision: Use float16 when possible\n",
    "    5. Batch processing: Don't load all data at once\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Multi-GPU Programming\n",
    "\n",
    "Scale to multiple GPUs for even more performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    n_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "    \n",
    "    print(f\"Multi-GPU Programming (Found {n_gpus} GPU(s))\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    if n_gpus > 1:\n",
    "        # Data Parallel: Same model, split data\n",
    "        print(\"\\nData Parallelism Example:\")\n",
    "        \n",
    "        # Simple model\n",
    "        class SimpleModel(torch.nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.linear = torch.nn.Linear(1000, 1000)\n",
    "            \n",
    "            def forward(self, x):\n",
    "                return self.linear(x)\n",
    "        \n",
    "        model = SimpleModel()\n",
    "        \n",
    "        # Wrap with DataParallel\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        model = model.cuda()\n",
    "        \n",
    "        # Forward pass automatically splits across GPUs\n",
    "        x = torch.rand(128, 1000).cuda()  # Batch size 128\n",
    "        output = model(x)  # Splits batch across GPUs\n",
    "        \n",
    "        print(f\"Model on {torch.cuda.device_count()} GPUs\")\n",
    "        print(f\"Input: {x.shape}, Output: {output.shape}\")\n",
    "        \n",
    "        # DistributedDataParallel (better for multi-node)\n",
    "        print(\"\\nFor production, use DistributedDataParallel:\")\n",
    "        print(\"\"\"\n",
    "        from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "        \n",
    "        # Initialize process group\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "        \n",
    "        # Wrap model\n",
    "        model = DDP(model, device_ids=[local_rank])\n",
    "        \"\"\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\"\"\n",
    "        Multi-GPU Strategies:\n",
    "        \n",
    "        1. Data Parallelism:\n",
    "           - Same model replicated on each GPU\n",
    "           - Different data batches\n",
    "           - Most common approach\n",
    "           \n",
    "        2. Model Parallelism:\n",
    "           - Split model across GPUs\n",
    "           - For models too large for single GPU\n",
    "           - More complex implementation\n",
    "           \n",
    "        3. Pipeline Parallelism:\n",
    "           - Different stages on different GPUs\n",
    "           - Overlaps computation\n",
    "           \n",
    "        Example:\n",
    "        model = torch.nn.DataParallel(model)  # Simple!\n",
    "        \"\"\")\n",
    "else:\n",
    "    print(\"Multi-GPU programming requires PyTorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Real-World GPU Applications\n",
    "\n",
    "### Application 1: Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU-accelerated image filtering\n",
    "import numpy as np\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    import cupy as cp\n",
    "    \n",
    "    # Create fake image (1920Ã—1080, RGB)\n",
    "    image_cpu = np.random.rand(1080, 1920, 3).astype(np.float32)\n",
    "    image_gpu = cp.asarray(image_cpu)\n",
    "    \n",
    "    # Gaussian blur kernel\n",
    "    def gaussian_blur_cpu(image):\n",
    "        \"\"\"CPU version.\"\"\"\n",
    "        kernel = np.array([[1, 2, 1],\n",
    "                          [2, 4, 2],\n",
    "                          [1, 2, 1]], dtype=np.float32) / 16\n",
    "        \n",
    "        # Simplified convolution (real version would use scipy)\n",
    "        return image  # Placeholder\n",
    "    \n",
    "    # Custom GPU kernel for blur\n",
    "    blur_kernel = cp.ElementwiseKernel(\n",
    "        'float32 x',\n",
    "        'float32 y',\n",
    "        'y = x * 0.8',  # Simplified\n",
    "        'blur'\n",
    "    )\n",
    "    \n",
    "    # Benchmark\n",
    "    n_iter = 100\n",
    "    \n",
    "    # GPU\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(n_iter):\n",
    "        result_gpu = blur_kernel(image_gpu)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    time_gpu = time.perf_counter() - start\n",
    "    \n",
    "    print(f\"Image Processing ({n_iter} iterations):\")\n",
    "    print(f\"GPU: {time_gpu:.4f}s ({time_gpu/n_iter*1000:.2f}ms per frame)\")\n",
    "    print(f\"FPS: {n_iter/time_gpu:.1f} frames/second\")\n",
    "    \n",
    "else:\n",
    "    print(\"GPU image processing can achieve 100+ FPS for HD video!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application 2: Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU-accelerated Monte Carlo\n",
    "if TORCH_AVAILABLE and torch.cuda.is_available():\n",
    "    def monte_carlo_pi_gpu(n_samples):\n",
    "        \"\"\"Estimate Ï€ using GPU Monte Carlo.\"\"\"\n",
    "        # Generate random points on GPU\n",
    "        x = torch.rand(n_samples, device='cuda')\n",
    "        y = torch.rand(n_samples, device='cuda')\n",
    "        \n",
    "        # Check if inside unit circle\n",
    "        inside = (x**2 + y**2) <= 1.0\n",
    "        \n",
    "        # Estimate Ï€\n",
    "        pi_estimate = 4.0 * inside.float().mean().item()\n",
    "        return pi_estimate\n",
    "    \n",
    "    # Run simulation\n",
    "    n_samples = 100_000_000  # 100 million!\n",
    "    \n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    start.record()\n",
    "    pi = monte_carlo_pi_gpu(n_samples)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    elapsed = start.elapsed_time(end) / 1000\n",
    "    \n",
    "    print(f\"\\nMonte Carlo Ï€ Estimation:\")\n",
    "    print(f\"Samples: {n_samples:,}\")\n",
    "    print(f\"Result: Ï€ â‰ˆ {pi:.6f} (true: 3.141593)\")\n",
    "    print(f\"Error: {abs(pi - 3.141593):.6f}\")\n",
    "    print(f\"Time: {elapsed:.4f}s\")\n",
    "    print(f\"Throughput: {n_samples/elapsed/1e6:.1f} million samples/second\")\n",
    "    \n",
    "else:\n",
    "    print(\"Monte Carlo simulations benefit hugely from GPU parallelism!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: GPU Optimization Techniques\n",
    "\n",
    "### 1. Coalesced Memory Access\n",
    "\n",
    "**Problem**: GPUs load memory in 128-byte chunks. Random access wastes bandwidth.\n",
    "\n",
    "```\n",
    "Bad (Strided):\n",
    "Thread 0: array[0]\n",
    "Thread 1: array[100]\n",
    "Thread 2: array[200]  â†’ Many memory transactions\n",
    "\n",
    "Good (Coalesced):\n",
    "Thread 0: array[0]\n",
    "Thread 1: array[1]\n",
    "Thread 2: array[2]  â†’ One memory transaction\n",
    "```\n",
    "\n",
    "### 2. Shared Memory\n",
    "\n",
    "Use fast shared memory (48-96 KB per SM) for data reuse.\n",
    "\n",
    "```python\n",
    "# CUDA kernel (pseudocode)\n",
    "__shared__ float tile[TILE_SIZE][TILE_SIZE];\n",
    "\n",
    "# Load from global â†’ shared (once)\n",
    "tile[ty][tx] = global_mem[...]\n",
    "__syncthreads()\n",
    "\n",
    "# Compute using shared memory (fast!)\n",
    "result = 0\n",
    "for (i = 0; i < TILE_SIZE; i++)\n",
    "    result += tile[ty][i] * tile[i][tx]\n",
    "```\n",
    "\n",
    "### 3. Occupancy Optimization\n",
    "\n",
    "**Occupancy** = Active warps / Maximum possible warps\n",
    "\n",
    "Higher occupancy hides memory latency better.\n",
    "\n",
    "**Factors**:\n",
    "- Threads per block (multiple of 32)\n",
    "- Registers per thread (fewer is better)\n",
    "- Shared memory usage (less is better)\n",
    "\n",
    "**Sweet spot**: 128-256 threads per block\n",
    "\n",
    "### 4. Kernel Fusion\n",
    "\n",
    "Combine multiple operations to reduce kernel launches.\n",
    "\n",
    "```python\n",
    "# Bad: Multiple kernel launches\n",
    "y = x + 1\n",
    "z = y * 2\n",
    "w = z - 3\n",
    "\n",
    "# Good: Fused operation\n",
    "w = (x + 1) * 2 - 3  # One kernel\n",
    "```\n",
    "\n",
    "### 5. Mixed Precision\n",
    "\n",
    "Use float16 when possible:\n",
    "- 2x less memory\n",
    "- 2x faster on Tensor Cores\n",
    "- Minimal accuracy loss\n",
    "\n",
    "```python\n",
    "model = model.half()  # Convert to float16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Exercises\n",
    "\n",
    "### Exercise 1: Vector Addition (Difficulty: â˜…â˜…â˜†â˜†â˜†)\n",
    "\n",
    "**Task**: Implement vector addition on GPU using CuPy or PyTorch:\n",
    "1. Create two large vectors (10 million elements)\n",
    "2. Add them on CPU and GPU\n",
    "3. Measure and compare performance\n",
    "4. Verify results are identical\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 2: Matrix Multiplication Optimization (Difficulty: â˜…â˜…â˜…â˜…â˜†)\n",
    "\n",
    "**Task**: Compare different matrix multiplication methods:\n",
    "1. Pure Python (nested loops)\n",
    "2. NumPy (CPU)\n",
    "3. CuPy or PyTorch (GPU)\n",
    "4. Mixed precision (float16 on GPU)\n",
    "\n",
    "Test with various sizes and plot speedup vs matrix size.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 3: Image Convolution (Difficulty: â˜…â˜…â˜…â˜…â˜†)\n",
    "\n",
    "**Task**: Implement 2D convolution on GPU:\n",
    "1. Load an image\n",
    "2. Apply various filters (blur, sharpen, edge detection)\n",
    "3. Compare CPU vs GPU performance\n",
    "4. Implement as custom CuPy kernel\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 4: Parallel Reduction (Difficulty: â˜…â˜…â˜…â˜…â˜†)\n",
    "\n",
    "**Task**: Implement parallel sum reduction:\n",
    "1. Create array of 100 million numbers\n",
    "2. Implement tree-based reduction\n",
    "3. Compare with built-in sum\n",
    "4. Measure throughput (GB/s)\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 5: Multi-GPU Training (Difficulty: â˜…â˜…â˜…â˜…â˜…)\n",
    "\n",
    "**Task**: If you have multiple GPUs:\n",
    "1. Create a simple neural network\n",
    "2. Implement data-parallel training\n",
    "3. Measure speedup vs single GPU\n",
    "4. Monitor GPU utilization\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 6: Memory Bandwidth Test (Difficulty: â˜…â˜…â˜…â˜†â˜†)\n",
    "\n",
    "**Task**: Measure GPU memory bandwidth:\n",
    "1. Copy large arrays between CPU and GPU\n",
    "2. Measure transfer speed (GB/s)\n",
    "3. Compare with GPU specs\n",
    "4. Identify bottlenecks (PCIe vs GPU memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 11: Self-Check Quiz\n",
    "\n",
    "### Question 1\n",
    "Why are GPUs faster than CPUs for parallel workloads?\n",
    "\n",
    "A) Higher clock speed  \n",
    "B) Thousands of cores for massive parallelism  \n",
    "C) Larger cache  \n",
    "D) Better branch prediction  \n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "B) Thousands of cores for massive parallelism\n",
    "\n",
    "**Explanation**: GPUs sacrifice per-core performance for massive parallelism, with thousands of simpler cores that excel at data-parallel tasks.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2\n",
    "What is the main bottleneck when using GPUs?\n",
    "\n",
    "A) Computation speed  \n",
    "B) Data transfer between CPU and GPU  \n",
    "C) Power consumption  \n",
    "D) Programming difficulty  \n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "B) Data transfer between CPU and GPU\n",
    "\n",
    "**Explanation**: PCIe bandwidth is limited (16-32 GB/s), much slower than GPU memory bandwidth (1000+ GB/s). Minimize CPU â†” GPU transfers!\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3\n",
    "What does synchronize() do in GPU programming?\n",
    "\n",
    "A) Copies data to GPU  \n",
    "B) Waits for GPU operations to complete  \n",
    "C) Frees GPU memory  \n",
    "D) Compiles kernels  \n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "B) Waits for GPU operations to complete\n",
    "\n",
    "**Explanation**: GPU operations are asynchronous. synchronize() ensures operations finish before continuing, necessary for accurate timing.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4\n",
    "When should you use float16 instead of float32 on GPU?\n",
    "\n",
    "A) Always, it's always faster  \n",
    "B) Never, it's less accurate  \n",
    "C) When memory is limited and precision loss is acceptable  \n",
    "D) Only for integer operations  \n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "C) When memory is limited and precision loss is acceptable\n",
    "\n",
    "**Explanation**: float16 uses half the memory and is faster on Tensor Cores, but has less precision. Good for deep learning, check carefully for other applications.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5\n",
    "What is DataParallel used for?\n",
    "\n",
    "A) Training different models on different GPUs  \n",
    "B) Splitting same model across multiple GPUs  \n",
    "C) Distributing data batches across multiple GPUs with same model  \n",
    "D) Compressing model size  \n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "C) Distributing data batches across multiple GPUs with same model\n",
    "\n",
    "**Explanation**: DataParallel replicates the model on each GPU and splits the batch across GPUs, then combines results. Most common multi-GPU approach.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **GPUs excel at parallelism**: Thousands of cores for data-parallel tasks\n",
    "2. **Transfer is expensive**: Keep data on GPU, minimize CPU â†” GPU copies\n",
    "3. **CuPy = NumPy on GPU**: Easiest way to start GPU computing\n",
    "4. **PyTorch for deep learning**: Seamless GPU acceleration\n",
    "5. **Memory is limited**: Monitor usage, use float16 when possible\n",
    "6. **Synchronization matters**: GPU ops are async, synchronize for timing\n",
    "7. **Batch operations**: Large batches amortize launch overhead\n",
    "8. **Coalesced access**: Contiguous memory access is critical\n",
    "9. **Multi-GPU scales**: DataParallel for easy multi-GPU training\n",
    "10. **Right tool for job**: GPU for parallel, CPU for sequential\n",
    "\n",
    "---\n",
    "\n",
    "## Common Mistakes\n",
    "\n",
    "1. **Frequent CPU â†” GPU transfers**: Keep data on GPU\n",
    "2. **Small workloads**: Overhead dominates, GPU slower than CPU\n",
    "3. **Forgetting synchronize()**: Timing without sync is wrong\n",
    "4. **Memory leaks**: Delete tensors, clear cache\n",
    "5. **Wrong precision**: float64 on GPU is slow\n",
    "6. **Sequential operations**: GPU needs parallelism\n",
    "7. **Not profiling**: Assumptions about bottlenecks\n",
    "8. **Ignoring occupancy**: Too many/few threads per block\n",
    "\n",
    "---\n",
    "\n",
    "## Pro Tips\n",
    "\n",
    "1. **Use Google Colab**: Free GPU access for learning\n",
    "2. **Profile with nvprof**: Identify kernel bottlenecks\n",
    "3. **Torch.cuda.amp**: Automatic mixed precision\n",
    "4. **Pin memory**: Faster CPU â†’ GPU transfers\n",
    "5. **Async transfers**: Overlap compute and transfer\n",
    "6. **NVIDIA Nsight**: Visual profiling tool\n",
    "7. **Benchmarking**: Warm up kernels, multiple runs\n",
    "8. **GPU utils**: nvidia-smi for monitoring\n",
    "\n",
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "**You're now ready for GPU-accelerated computing!**\n",
    "\n",
    "**Advanced Topics**:\n",
    "1. **CUDA C++**: Write custom kernels for maximum performance\n",
    "2. **JAX**: Composable transformations for ML research\n",
    "3. **TensorRT**: Optimize models for inference\n",
    "4. **Distributed Training**: Multi-node GPU clusters\n",
    "5. **GPU Optimization**: Advanced memory patterns\n",
    "\n",
    "**Projects to Build**:\n",
    "- Real-time image processing pipeline\n",
    "- GPU-accelerated data science workflow\n",
    "- Deep learning model from scratch\n",
    "- Physics simulation (N-body, fluid dynamics)\n",
    "- Cryptocurrency miner (educational!)\n",
    "\n",
    "**Remember**: GPUs are powerful but not magic. Profile first, optimize bottlenecks, and use the right tool for each task!\n",
    "\n",
    "**Congratulations on completing the Education Playground curriculum!** ðŸŽ“ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
