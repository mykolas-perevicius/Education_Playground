{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Level: High-Performance Python Computing\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "**The Problem**: Python is often criticized for being \"slow\" compared to C/C++. In reality, proper optimization can make Python competitive for many tasks, and understanding performance is crucial for:\n",
    "- Processing large datasets (millions/billions of records)\n",
    "- Real-time systems (trading, gaming, streaming)\n",
    "- Scientific computing and simulations\n",
    "- Machine learning model training\n",
    "- Web services handling millions of requests\n",
    "\n",
    "**Why This Matters**:\n",
    "- **Cost Savings**: 10x faster code = 10x fewer servers\n",
    "- **User Experience**: Fast responses keep users engaged\n",
    "- **Scalability**: Efficient code handles more load\n",
    "- **Energy**: Less computation = lower power consumption\n",
    "\n",
    "**What You'll Learn**:\n",
    "- How to profile and find bottlenecks\n",
    "- Memory and CPU optimization techniques\n",
    "- Just-In-Time (JIT) compilation with Numba\n",
    "- Vectorization with NumPy\n",
    "- Multiprocessing and threading\n",
    "- Asynchronous I/O for concurrency\n",
    "- Cython for C-speed Python\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Profiling - Finding the Bottleneck\n",
    "\n",
    "**Golden Rule**: \"Premature optimization is the root of all evil\" - Donald Knuth\n",
    "\n",
    "Always profile first! 90% of execution time is typically spent in 10% of the code.\n",
    "\n",
    "### Profiling Tools\n",
    "\n",
    "1. **time/timeit**: Simple timing\n",
    "2. **cProfile**: Built-in Python profiler\n",
    "3. **line_profiler**: Line-by-line profiling\n",
    "4. **memory_profiler**: Track memory usage\n",
    "5. **py-spy**: Sampling profiler (no code changes needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cProfile\n",
    "import pstats\n",
    "from io import StringIO\n",
    "\n",
    "# Example: Slow vs Fast String Concatenation\n",
    "\n",
    "def slow_concat(n):\n",
    "    \"\"\"Slow: O(nÂ²) due to string immutability.\"\"\"\n",
    "    result = \"\"\n",
    "    for i in range(n):\n",
    "        result += str(i)  # Creates new string each time\n",
    "    return result\n",
    "\n",
    "def fast_concat(n):\n",
    "    \"\"\"Fast: O(n) using list join.\"\"\"\n",
    "    parts = []\n",
    "    for i in range(n):\n",
    "        parts.append(str(i))\n",
    "    return ''.join(parts)\n",
    "\n",
    "# Timing comparison\n",
    "n = 10000\n",
    "\n",
    "start = time.perf_counter()\n",
    "slow_result = slow_concat(n)\n",
    "slow_time = time.perf_counter() - start\n",
    "\n",
    "start = time.perf_counter()\n",
    "fast_result = fast_concat(n)\n",
    "fast_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Slow concat: {slow_time:.4f}s\")\n",
    "print(f\"Fast concat: {fast_time:.4f}s\")\n",
    "print(f\"Speedup: {slow_time/fast_time:.2f}x faster\")\n",
    "\n",
    "# Using timeit for more accurate measurements\n",
    "import timeit\n",
    "\n",
    "slow_avg = timeit.timeit(lambda: slow_concat(1000), number=100) / 100\n",
    "fast_avg = timeit.timeit(lambda: fast_concat(1000), number=100) / 100\n",
    "\n",
    "print(f\"\\nAverage over 100 runs (n=1000):\")\n",
    "print(f\"Slow: {slow_avg*1000:.2f}ms\")\n",
    "print(f\"Fast: {fast_avg*1000:.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cProfile Example: Finding hotspots\n",
    "\n",
    "def fibonacci_recursive(n):\n",
    "    \"\"\"Inefficient recursive fibonacci.\"\"\"\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci_recursive(n-1) + fibonacci_recursive(n-2)\n",
    "\n",
    "def fibonacci_iterative(n):\n",
    "    \"\"\"Efficient iterative fibonacci.\"\"\"\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    a, b = 0, 1\n",
    "    for _ in range(n-1):\n",
    "        a, b = b, a + b\n",
    "    return b\n",
    "\n",
    "# Profile the recursive version\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "result = fibonacci_recursive(25)\n",
    "profiler.disable()\n",
    "\n",
    "# Print stats\n",
    "s = StringIO()\n",
    "ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n",
    "ps.print_stats(10)  # Top 10 functions\n",
    "print(\"Recursive Fibonacci Profile:\")\n",
    "print(s.getvalue())\n",
    "\n",
    "# Compare execution times\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "n = 30\n",
    "\n",
    "start = time.perf_counter()\n",
    "rec_result = fibonacci_recursive(n)\n",
    "rec_time = time.perf_counter() - start\n",
    "\n",
    "start = time.perf_counter()\n",
    "iter_result = fibonacci_iterative(n)\n",
    "iter_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Recursive: {rec_time:.4f}s\")\n",
    "print(f\"Iterative: {iter_time:.6f}s\")\n",
    "print(f\"Speedup: {rec_time/iter_time:.0f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Profiling Insights\n",
    "\n",
    "**What to Look For:**\n",
    "1. **High cumulative time**: Functions called often or running long\n",
    "2. **Number of calls**: Unexpectedly high call counts indicate inefficiency\n",
    "3. **Per-call time**: Individual function slowness\n",
    "\n",
    "**Common Bottlenecks:**\n",
    "- String concatenation in loops\n",
    "- Nested loops (O(nÂ²) or worse)\n",
    "- Repeated file I/O\n",
    "- Database queries in loops (N+1 problem)\n",
    "- Inefficient algorithms (bubble sort vs quicksort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Memory Optimization\n",
    "\n",
    "Memory is often the limiting factor in data processing. Understanding memory usage prevents crashes and improves performance.\n",
    "\n",
    "### Memory Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import namedtuple\n",
    "\n",
    "# Memory comparison: List vs Tuple vs Generator\n",
    "\n",
    "def memory_usage_demo():\n",
    "    n = 1_000_000\n",
    "    \n",
    "    # List: stores all elements in memory\n",
    "    list_data = [i for i in range(n)]\n",
    "    list_size = sys.getsizeof(list_data)\n",
    "    \n",
    "    # Tuple: slightly more memory efficient\n",
    "    tuple_data = tuple(i for i in range(n))\n",
    "    tuple_size = sys.getsizeof(tuple_data)\n",
    "    \n",
    "    # Generator: minimal memory (just stores state)\n",
    "    gen_data = (i for i in range(n))\n",
    "    gen_size = sys.getsizeof(gen_data)\n",
    "    \n",
    "    print(f\"Memory Usage for {n:,} integers:\")\n",
    "    print(f\"List:      {list_size:>12,} bytes ({list_size/1024/1024:.2f} MB)\")\n",
    "    print(f\"Tuple:     {tuple_size:>12,} bytes ({tuple_size/1024/1024:.2f} MB)\")\n",
    "    print(f\"Generator: {gen_size:>12,} bytes ({gen_size/1024:.2f} KB)\")\n",
    "    print(f\"\\nGenerator is {list_size/gen_size:.0f}x more memory efficient!\")\n",
    "\n",
    "memory_usage_demo()\n",
    "\n",
    "# Slots for memory-efficient classes\n",
    "class Point:\n",
    "    \"\"\"Regular class with __dict__.\"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "class PointSlots:\n",
    "    \"\"\"Memory-efficient class with __slots__.\"\"\"\n",
    "    __slots__ = ['x', 'y']\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "# Named tuple: even more efficient\n",
    "PointNT = namedtuple('PointNT', ['x', 'y'])\n",
    "\n",
    "# Compare memory usage\n",
    "p1 = Point(1, 2)\n",
    "p2 = PointSlots(1, 2)\n",
    "p3 = PointNT(1, 2)\n",
    "\n",
    "print(\"\\nClass Memory Comparison:\")\n",
    "print(f\"Regular class: {sys.getsizeof(p1) + sys.getsizeof(vars(p1))} bytes\")\n",
    "print(f\"With __slots__: {sys.getsizeof(p2)} bytes\")\n",
    "print(f\"Named tuple: {sys.getsizeof(p3)} bytes\")\n",
    "\n",
    "# For millions of objects, this difference is massive!\n",
    "n_objects = 1_000_000\n",
    "regular_memory = n_objects * (sys.getsizeof(p1) + sys.getsizeof(vars(p1)))\n",
    "slots_memory = n_objects * sys.getsizeof(p2)\n",
    "\n",
    "print(f\"\\nFor {n_objects:,} objects:\")\n",
    "print(f\"Regular: {regular_memory/1024/1024:.2f} MB\")\n",
    "print(f\"Slots: {slots_memory/1024/1024:.2f} MB\")\n",
    "print(f\"Savings: {(regular_memory - slots_memory)/1024/1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Optimization Techniques\n",
    "\n",
    "1. **Use generators** for large sequences\n",
    "2. **__slots__** for classes with many instances\n",
    "3. **del** unused variables in long-running functions\n",
    "4. **gc.collect()** to force garbage collection\n",
    "5. **Array/NumPy** for homogeneous numeric data\n",
    "6. **Memory-mapped files** for huge datasets\n",
    "7. **Lazy loading** - load data only when needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: NumPy Vectorization\n",
    "\n",
    "NumPy operations are 10-100x faster than pure Python because they:\n",
    "1. Use compiled C code\n",
    "2. Operate on contiguous memory blocks\n",
    "3. Avoid Python interpreter overhead\n",
    "4. Use SIMD instructions when possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: Sum of squares\n",
    "\n",
    "def sum_of_squares_python(n):\n",
    "    \"\"\"Pure Python implementation.\"\"\"\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        total += i ** 2\n",
    "    return total\n",
    "\n",
    "def sum_of_squares_numpy(n):\n",
    "    \"\"\"Vectorized NumPy implementation.\"\"\"\n",
    "    arr = np.arange(n)\n",
    "    return np.sum(arr ** 2)\n",
    "\n",
    "# Benchmark\n",
    "n = 1_000_000\n",
    "\n",
    "# Python version\n",
    "start = time.perf_counter()\n",
    "result_py = sum_of_squares_python(n)\n",
    "time_py = time.perf_counter() - start\n",
    "\n",
    "# NumPy version\n",
    "start = time.perf_counter()\n",
    "result_np = sum_of_squares_numpy(n)\n",
    "time_np = time.perf_counter() - start\n",
    "\n",
    "print(f\"Sum of squares for {n:,} numbers:\")\n",
    "print(f\"Python: {time_py:.4f}s\")\n",
    "print(f\"NumPy:  {time_np:.4f}s\")\n",
    "print(f\"Speedup: {time_py/time_np:.1f}x faster!\")\n",
    "\n",
    "# More complex example: Distance matrix\n",
    "def euclidean_distance_python(points):\n",
    "    \"\"\"Compute pairwise distances - Python loops.\"\"\"\n",
    "    n = len(points)\n",
    "    distances = [[0.0] * n for _ in range(n)]\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            dx = points[i][0] - points[j][0]\n",
    "            dy = points[i][1] - points[j][1]\n",
    "            distances[i][j] = (dx**2 + dy**2) ** 0.5\n",
    "    \n",
    "    return distances\n",
    "\n",
    "def euclidean_distance_numpy(points):\n",
    "    \"\"\"Compute pairwise distances - NumPy broadcasting.\"\"\"\n",
    "    # Reshape for broadcasting: (n, 1, 2) - (1, n, 2) = (n, n, 2)\n",
    "    diff = points[:, np.newaxis, :] - points[np.newaxis, :, :]\n",
    "    return np.sqrt(np.sum(diff ** 2, axis=2))\n",
    "\n",
    "# Test with random points\n",
    "np.random.seed(42)\n",
    "n_points = 1000\n",
    "points_list = [[np.random.rand(), np.random.rand()] for _ in range(n_points)]\n",
    "points_array = np.array(points_list)\n",
    "\n",
    "# Benchmark (use smaller n for Python version)\n",
    "n_test = 100\n",
    "\n",
    "start = time.perf_counter()\n",
    "dist_py = euclidean_distance_python(points_list[:n_test])\n",
    "time_py = time.perf_counter() - start\n",
    "\n",
    "start = time.perf_counter()\n",
    "dist_np = euclidean_distance_numpy(points_array[:n_test])\n",
    "time_np = time.perf_counter() - start\n",
    "\n",
    "print(f\"\\nDistance matrix for {n_test} points:\")\n",
    "print(f\"Python: {time_py:.4f}s\")\n",
    "print(f\"NumPy:  {time_np:.6f}s\")\n",
    "print(f\"Speedup: {time_py/time_np:.1f}x faster!\")\n",
    "\n",
    "# NumPy can handle much larger datasets\n",
    "start = time.perf_counter()\n",
    "dist_large = euclidean_distance_numpy(points_array)  # All 1000 points\n",
    "time_large = time.perf_counter() - start\n",
    "print(f\"\\nNumPy with 1000 points: {time_large:.4f}s\")\n",
    "print(f\"Creates {1000*1000:,} distance calculations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization Best Practices\n",
    "\n",
    "**Do:**\n",
    "- Use NumPy ufuncs (universal functions) instead of loops\n",
    "- Leverage broadcasting for element-wise operations\n",
    "- Use built-in aggregations (sum, mean, max, etc.)\n",
    "- Preallocate arrays when possible\n",
    "\n",
    "**Don't:**\n",
    "- Loop over NumPy arrays (defeats the purpose)\n",
    "- Use `.append()` in loops (very slow)\n",
    "- Mix Python loops with NumPy operations\n",
    "- Create unnecessary array copies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Just-In-Time Compilation with Numba\n",
    "\n",
    "Numba compiles Python functions to machine code at runtime, achieving C-like performance with minimal code changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install: pip install numba\n",
    "try:\n",
    "    from numba import jit, njit, prange\n",
    "    NUMBA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Numba not installed. Run: pip install numba\")\n",
    "    NUMBA_AVAILABLE = False\n",
    "\n",
    "if NUMBA_AVAILABLE:\n",
    "    # Example 1: Simple numerical computation\n",
    "    \n",
    "    def monte_carlo_pi_python(n_samples):\n",
    "        \"\"\"Estimate Ï€ using Monte Carlo - Pure Python.\"\"\"\n",
    "        inside_circle = 0\n",
    "        for i in range(n_samples):\n",
    "            x = np.random.random()\n",
    "            y = np.random.random()\n",
    "            if x*x + y*y <= 1.0:\n",
    "                inside_circle += 1\n",
    "        return 4.0 * inside_circle / n_samples\n",
    "    \n",
    "    @jit(nopython=True)  # Compile to machine code\n",
    "    def monte_carlo_pi_numba(n_samples):\n",
    "        \"\"\"Estimate Ï€ using Monte Carlo - Numba JIT.\"\"\"\n",
    "        inside_circle = 0\n",
    "        for i in range(n_samples):\n",
    "            x = np.random.random()\n",
    "            y = np.random.random()\n",
    "            if x*x + y*y <= 1.0:\n",
    "                inside_circle += 1\n",
    "        return 4.0 * inside_circle / n_samples\n",
    "    \n",
    "    # Benchmark\n",
    "    n = 10_000_000\n",
    "    \n",
    "    # Warm up Numba (first call compiles)\n",
    "    _ = monte_carlo_pi_numba(100)\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    pi_python = monte_carlo_pi_python(n)\n",
    "    time_py = time.perf_counter() - start\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    pi_numba = monte_carlo_pi_numba(n)\n",
    "    time_numba = time.perf_counter() - start\n",
    "    \n",
    "    print(f\"Monte Carlo Ï€ estimation ({n:,} samples):\")\n",
    "    print(f\"Python: {pi_python:.6f} in {time_py:.4f}s\")\n",
    "    print(f\"Numba:  {pi_numba:.6f} in {time_numba:.4f}s\")\n",
    "    print(f\"Speedup: {time_py/time_numba:.1f}x faster!\")\n",
    "    \n",
    "    # Example 2: Parallel execution\n",
    "    \n",
    "    @njit(parallel=True)\n",
    "    def parallel_sum_of_squares(n):\n",
    "        \"\"\"Parallel computation with Numba.\"\"\"\n",
    "        total = 0\n",
    "        for i in prange(n):  # Parallel range\n",
    "            total += i * i\n",
    "        return total\n",
    "    \n",
    "    # Compare with sequential\n",
    "    @njit\n",
    "    def sequential_sum_of_squares(n):\n",
    "        \"\"\"Sequential Numba computation.\"\"\"\n",
    "        total = 0\n",
    "        for i in range(n):\n",
    "            total += i * i\n",
    "        return total\n",
    "    \n",
    "    # Warm up\n",
    "    _ = parallel_sum_of_squares(100)\n",
    "    _ = sequential_sum_of_squares(100)\n",
    "    \n",
    "    n = 100_000_000\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    result_seq = sequential_sum_of_squares(n)\n",
    "    time_seq = time.perf_counter() - start\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    result_par = parallel_sum_of_squares(n)\n",
    "    time_par = time.perf_counter() - start\n",
    "    \n",
    "    print(f\"\\nSum of squares ({n:,}):\")\n",
    "    print(f\"Sequential: {time_seq:.4f}s\")\n",
    "    print(f\"Parallel:   {time_par:.4f}s\")\n",
    "    print(f\"Speedup: {time_seq/time_par:.1f}x faster!\")\n",
    "else:\n",
    "    print(\"Numba examples skipped (not installed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numba Guidelines\n",
    "\n",
    "**When to use Numba:**\n",
    "- Numerical computations with loops\n",
    "- Array operations that can't be vectorized\n",
    "- Physics simulations, Monte Carlo methods\n",
    "- Signal processing algorithms\n",
    "\n",
    "**Limitations:**\n",
    "- Limited Python feature support (no complex objects)\n",
    "- First call is slow (compilation)\n",
    "- Not useful for I/O-bound code\n",
    "- NumPy is still faster for pure vectorized ops\n",
    "\n",
    "**Best Practices:**\n",
    "- Use `nopython=True` (or `@njit`) for maximum speed\n",
    "- Avoid object mode fallback\n",
    "- Use `parallel=True` for CPU-bound loops\n",
    "- Cache compiled functions with `cache=True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Multiprocessing and Threading\n",
    "\n",
    "Python has two main concurrency models:\n",
    "- **Threading**: For I/O-bound tasks (GIL limits CPU parallelism)\n",
    "- **Multiprocessing**: For CPU-bound tasks (bypasses GIL)\n",
    "\n",
    "### The Global Interpreter Lock (GIL)\n",
    "\n",
    "The GIL prevents multiple threads from executing Python code simultaneously. This means:\n",
    "- **Threads**: Good for I/O (network, disk)\n",
    "- **Processes**: Needed for CPU parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "\n",
    "# CPU-bound task for testing\n",
    "def compute_intensive(n):\n",
    "    \"\"\"Simulate CPU-intensive work.\"\"\"\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        total += i ** 2\n",
    "    return total\n",
    "\n",
    "# Sequential baseline\n",
    "def sequential_processing(tasks):\n",
    "    \"\"\"Process tasks sequentially.\"\"\"\n",
    "    return [compute_intensive(n) for n in tasks]\n",
    "\n",
    "# Threading (won't help for CPU-bound due to GIL)\n",
    "def threaded_processing(tasks, max_workers=4):\n",
    "    \"\"\"Process tasks with threads.\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        return list(executor.map(compute_intensive, tasks))\n",
    "\n",
    "# Multiprocessing (bypasses GIL)\n",
    "def multiprocess_processing(tasks, max_workers=4):\n",
    "    \"\"\"Process tasks with separate processes.\"\"\"\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        return list(executor.map(compute_intensive, tasks))\n",
    "\n",
    "# Benchmark\n",
    "if __name__ == \"__main__\":\n",
    "    tasks = [5_000_000] * 8  # 8 CPU-intensive tasks\n",
    "\n",
    "    print(\"Processing 8 CPU-intensive tasks:\")\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    result_seq = sequential_processing(tasks)\n",
    "    time_seq = time.perf_counter() - start\n",
    "    print(f\"Sequential:      {time_seq:.2f}s\")\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    result_thread = threaded_processing(tasks, max_workers=4)\n",
    "    time_thread = time.perf_counter() - start\n",
    "    print(f\"Threading (4):   {time_thread:.2f}s (Speedup: {time_seq/time_thread:.2f}x)\")\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    result_multi = multiprocess_processing(tasks, max_workers=4)\n",
    "    time_multi = time.perf_counter() - start\n",
    "    print(f\"Multiprocess(4): {time_multi:.2f}s (Speedup: {time_seq/time_multi:.2f}x)\")\n",
    "\n",
    "    print(\"\\nKey Insight: Multiprocessing provides real CPU parallelism!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O-bound example: Threading shines here\n",
    "import urllib.request\n",
    "\n",
    "def fetch_url(url):\n",
    "    \"\"\"Simulate I/O-bound operation.\"\"\"\n",
    "    try:\n",
    "        with urllib.request.urlopen(url, timeout=5) as response:\n",
    "            return len(response.read())\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "urls = [\n",
    "    'http://www.example.com',\n",
    "    'http://www.python.org',\n",
    "    'http://www.github.com',\n",
    "    'http://www.stackoverflow.com',\n",
    "] * 3  # 12 requests total\n",
    "\n",
    "# Sequential\n",
    "print(\"Fetching 12 URLs:\")\n",
    "start = time.perf_counter()\n",
    "sizes_seq = [fetch_url(url) for url in urls]\n",
    "time_seq = time.perf_counter() - start\n",
    "print(f\"Sequential: {time_seq:.2f}s\")\n",
    "\n",
    "# Threaded (much better for I/O)\n",
    "start = time.perf_counter()\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    sizes_thread = list(executor.map(fetch_url, urls))\n",
    "time_thread = time.perf_counter() - start\n",
    "print(f\"Threading:  {time_thread:.2f}s (Speedup: {time_seq/time_thread:.2f}x)\")\n",
    "\n",
    "print(\"\\nFor I/O-bound tasks, threading is ideal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concurrency Decision Tree\n",
    "\n",
    "```\n",
    "Is your task CPU-bound or I/O-bound?\n",
    "â”‚\n",
    "â”œâ”€ CPU-bound (computation, data processing)\n",
    "â”‚  â”‚\n",
    "â”‚  â”œâ”€ Vectorizable? â†’ Use NumPy\n",
    "â”‚  â”œâ”€ Loops required? â†’ Use Numba\n",
    "â”‚  â””â”€ Multiple cores? â†’ Use multiprocessing\n",
    "â”‚\n",
    "â””â”€ I/O-bound (network, disk, database)\n",
    "   â”‚\n",
    "   â”œâ”€ Many concurrent operations? â†’ Use asyncio\n",
    "   â””â”€ Mixed I/O and CPU? â†’ Use threading\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Asynchronous I/O with asyncio\n",
    "\n",
    "asyncio enables concurrent I/O without threading overhead. Perfect for:\n",
    "- Web scraping\n",
    "- API calls\n",
    "- Database queries\n",
    "- Real-time applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Basic async example\n",
    "async def fetch_data(url, delay):\n",
    "    \"\"\"Simulate async data fetching.\"\"\"\n",
    "    print(f\"Fetching {url}...\")\n",
    "    await asyncio.sleep(delay)  # Simulate network delay\n",
    "    print(f\"Completed {url}\")\n",
    "    return f\"Data from {url}\"\n",
    "\n",
    "async def sequential_async():\n",
    "    \"\"\"Fetch sequentially (awaits each).\"\"\"\n",
    "    result1 = await fetch_data(\"API1\", 2)\n",
    "    result2 = await fetch_data(\"API2\", 2)\n",
    "    result3 = await fetch_data(\"API3\", 2)\n",
    "    return [result1, result2, result3]\n",
    "\n",
    "async def concurrent_async():\n",
    "    \"\"\"Fetch concurrently (gather).\"\"\"\n",
    "    tasks = [\n",
    "        fetch_data(\"API1\", 2),\n",
    "        fetch_data(\"API2\", 2),\n",
    "        fetch_data(\"API3\", 2),\n",
    "    ]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "# Run in Jupyter (handles event loop)\n",
    "print(\"Sequential async (total: ~6 seconds):\")\n",
    "start = time.perf_counter()\n",
    "results_seq = await sequential_async()\n",
    "time_seq = time.perf_counter() - start\n",
    "print(f\"Completed in {time_seq:.2f}s\\n\")\n",
    "\n",
    "print(\"Concurrent async (total: ~2 seconds):\")\n",
    "start = time.perf_counter()\n",
    "results_con = await concurrent_async()\n",
    "time_con = time.perf_counter() - start\n",
    "print(f\"Completed in {time_con:.2f}s\")\n",
    "print(f\"Speedup: {time_seq/time_con:.1f}x faster!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world async pattern: Rate-limited API calls\n",
    "\n",
    "async def rate_limited_fetch(semaphore, url, delay):\n",
    "    \"\"\"Fetch with concurrency limit.\"\"\"\n",
    "    async with semaphore:  # Limit concurrent requests\n",
    "        print(f\"Starting {url}\")\n",
    "        await asyncio.sleep(delay)\n",
    "        print(f\"Finished {url}\")\n",
    "        return f\"Data from {url}\"\n",
    "\n",
    "async def fetch_many_with_limit(urls, max_concurrent=3):\n",
    "    \"\"\"Fetch many URLs with concurrency limit.\"\"\"\n",
    "    semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    tasks = [rate_limited_fetch(semaphore, url, 1) for url in urls]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "# Fetch 10 URLs but only 3 at a time\n",
    "urls = [f\"URL{i}\" for i in range(10)]\n",
    "\n",
    "print(\"Fetching 10 URLs with max 3 concurrent:\")\n",
    "start = time.perf_counter()\n",
    "results = await fetch_many_with_limit(urls, max_concurrent=3)\n",
    "elapsed = time.perf_counter() - start\n",
    "\n",
    "print(f\"\\nCompleted in {elapsed:.2f}s\")\n",
    "print(f\"With 3 concurrent, 10 URLs take ~4 batches: ceil(10/3) = 4 seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async Best Practices\n",
    "\n",
    "**Do:**\n",
    "- Use async for I/O-bound concurrent operations\n",
    "- Use `asyncio.gather()` for parallel awaits\n",
    "- Use `asyncio.Semaphore()` for rate limiting\n",
    "- Use `aiohttp` for async HTTP requests\n",
    "\n",
    "**Don't:**\n",
    "- Use async for CPU-bound tasks (blocks event loop)\n",
    "- Mix sync and async APIs carelessly\n",
    "- Forget to await async functions\n",
    "- Use `time.sleep()` instead of `asyncio.sleep()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Cython - C Extension Speed\n",
    "\n",
    "Cython compiles Python-like code to C extensions for maximum performance.\n",
    "\n",
    "**When to use Cython:**\n",
    "- Need C-level performance\n",
    "- Interfacing with C libraries\n",
    "- Tight loops that can't be vectorized\n",
    "- Numba isn't enough\n",
    "\n",
    "### Cython Example (Pseudocode)\n",
    "\n",
    "```cython\n",
    "# fibonacci.pyx\n",
    "def fib_cython(int n):\n",
    "    cdef int a = 0, b = 1, temp\n",
    "    cdef int i\n",
    "    \n",
    "    for i in range(n-1):\n",
    "        temp = a\n",
    "        a = b\n",
    "        b = temp + b\n",
    "    \n",
    "    return b\n",
    "```\n",
    "\n",
    "Compile with:\n",
    "```bash\n",
    "cython fibonacci.pyx\n",
    "gcc -shared -pthread -fPIC -fwrapv -O2 -Wall -fno-strict-aliasing \\\n",
    "    -I/usr/include/python3.x \\\n",
    "    -o fibonacci.so fibonacci.c\n",
    "```\n",
    "\n",
    "**Performance**: 10-100x faster than pure Python for numerical code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Performance Optimization Checklist\n",
    "\n",
    "### 1. Profile First âœ“\n",
    "- Use cProfile to find hotspots\n",
    "- Use line_profiler for detailed analysis\n",
    "- Use memory_profiler for memory issues\n",
    "\n",
    "### 2. Algorithm Optimization âœ“\n",
    "- Choose right data structure (dict vs list vs set)\n",
    "- Use appropriate algorithm (O(n log n) vs O(nÂ²))\n",
    "- Cache repeated calculations\n",
    "- Avoid premature optimization\n",
    "\n",
    "### 3. Data Structure Choice âœ“\n",
    "- Use `set` for membership testing\n",
    "- Use `dict` for lookups\n",
    "- Use `collections.deque` for queues\n",
    "- Use `collections.Counter` for counting\n",
    "\n",
    "### 4. Vectorization âœ“\n",
    "- Replace loops with NumPy operations\n",
    "- Use broadcasting for element-wise ops\n",
    "- Avoid mixing loops and NumPy\n",
    "\n",
    "### 5. Compilation âœ“\n",
    "- Use Numba for numerical code\n",
    "- Use Cython for C-level performance\n",
    "- Consider PyPy for long-running services\n",
    "\n",
    "### 6. Concurrency âœ“\n",
    "- Multiprocessing for CPU-bound\n",
    "- Threading for I/O-bound (with caution)\n",
    "- Asyncio for high-concurrency I/O\n",
    "\n",
    "### 7. Memory Optimization âœ“\n",
    "- Use generators for large sequences\n",
    "- Use __slots__ for many objects\n",
    "- Del unused variables\n",
    "- Use memory-mapped files for huge data\n",
    "\n",
    "### 8. I/O Optimization âœ“\n",
    "- Batch database queries\n",
    "- Use connection pooling\n",
    "- Cache file reads\n",
    "- Use binary formats (pickle, parquet)\n",
    "\n",
    "### 9. String Operations âœ“\n",
    "- Use join() instead of += in loops\n",
    "- Use f-strings for formatting\n",
    "- Use str.format() for templates\n",
    "- Compile regex patterns once\n",
    "\n",
    "### 10. Measure Results âœ“\n",
    "- Always benchmark optimizations\n",
    "- Use realistic data sizes\n",
    "- Test edge cases\n",
    "- Document performance characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Exercises\n",
    "\n",
    "### Exercise 1: Optimize Matrix Multiplication (Difficulty: â˜…â˜…â˜…â˜†â˜†)\n",
    "\n",
    "**Task**: Implement matrix multiplication three ways and compare performance:\n",
    "1. Pure Python with nested loops\n",
    "2. NumPy vectorized\n",
    "3. Numba JIT compiled\n",
    "\n",
    "Test with 500Ã—500 matrices and measure speedup.\n",
    "\n",
    "**Expected outcome**: NumPy should be 50-100x faster, Numba similar to NumPy.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 2: Parallel File Processing (Difficulty: â˜…â˜…â˜…â˜…â˜†)\n",
    "\n",
    "**Task**: Process 100 text files in parallel:\n",
    "1. Read each file\n",
    "2. Count word frequency\n",
    "3. Return top 10 words\n",
    "\n",
    "Implement using:\n",
    "- Sequential processing\n",
    "- ThreadPoolExecutor\n",
    "- ProcessPoolExecutor\n",
    "\n",
    "Which is faster and why?\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 3: Memory-Efficient Data Processing (Difficulty: â˜…â˜…â˜…â˜†â˜†)\n",
    "\n",
    "**Task**: Process a 1GB CSV file:\n",
    "1. Read and parse without loading entire file\n",
    "2. Calculate statistics (mean, median, std)\n",
    "3. Use generator-based approach\n",
    "\n",
    "Compare memory usage vs loading all data.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 4: Async Web Scraper (Difficulty: â˜…â˜…â˜…â˜…â˜†)\n",
    "\n",
    "**Task**: Build an async web scraper:\n",
    "1. Fetch 50 URLs concurrently\n",
    "2. Limit to 10 concurrent connections\n",
    "3. Handle errors gracefully\n",
    "4. Track completion time\n",
    "\n",
    "Compare with sequential approach.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 5: Cache Decorator (Difficulty: â˜…â˜…â˜…â˜†â˜†)\n",
    "\n",
    "**Task**: Implement a caching decorator with:\n",
    "1. LRU cache (max size)\n",
    "2. TTL (time-to-live)\n",
    "3. Cache statistics (hits/misses)\n",
    "\n",
    "Test with expensive function calls.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 6: Profile and Optimize (Difficulty: â˜…â˜…â˜…â˜…â˜…)\n",
    "\n",
    "**Task**: Given this slow code, optimize it:\n",
    "\n",
    "```python\n",
    "def slow_function(data):\n",
    "    result = []\n",
    "    for item in data:\n",
    "        if item not in result:\n",
    "            result.append(item)\n",
    "    return sorted(result)\n",
    "```\n",
    "\n",
    "1. Profile to find bottlenecks\n",
    "2. Optimize (hint: use set)\n",
    "3. Measure speedup\n",
    "4. Document complexity improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Self-Check Quiz\n",
    "\n",
    "### Question 1\n",
    "What is the primary advantage of using generators over lists for large datasets?\n",
    "\n",
    "A) Generators are faster  \n",
    "B) Generators use less memory  \n",
    "C) Generators are easier to write  \n",
    "D) Generators can be pickled  \n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "B) Generators use less memory\n",
    "\n",
    "**Explanation**: Generators compute values on-demand and don't store all elements in memory, making them ideal for large or infinite sequences.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2\n",
    "When should you use multiprocessing instead of threading in Python?\n",
    "\n",
    "A) For I/O-bound tasks  \n",
    "B) For CPU-bound tasks  \n",
    "C) When you need shared memory  \n",
    "D) When you want faster startup  \n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "B) For CPU-bound tasks\n",
    "\n",
    "**Explanation**: The GIL prevents threads from executing Python code in parallel on multiple cores. Multiprocessing bypasses the GIL by using separate processes.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3\n",
    "What makes NumPy significantly faster than pure Python for numerical operations?\n",
    "\n",
    "A) It's written in a faster language  \n",
    "B) It uses compiled C code and operates on contiguous memory  \n",
    "C) It automatically parallelizes operations  \n",
    "D) It caches all results  \n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "B) It uses compiled C code and operates on contiguous memory\n",
    "\n",
    "**Explanation**: NumPy operations are implemented in C and work on contiguous memory blocks, avoiding Python interpreter overhead and enabling CPU cache efficiency.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4\n",
    "What is the main limitation of asyncio in Python?\n",
    "\n",
    "A) It's too complex  \n",
    "B) It doesn't work with CPU-bound tasks  \n",
    "C) It requires special hardware  \n",
    "D) It only works on Linux  \n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "B) It doesn't work with CPU-bound tasks\n",
    "\n",
    "**Explanation**: Asyncio is designed for I/O-bound concurrency. CPU-bound tasks block the event loop, preventing other tasks from running.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5\n",
    "What does the @jit decorator in Numba do?\n",
    "\n",
    "A) Distributes code across multiple machines  \n",
    "B) Compiles Python functions to machine code at runtime  \n",
    "C) Caches function results  \n",
    "D) Converts code to JavaScript  \n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "B) Compiles Python functions to machine code at runtime\n",
    "\n",
    "**Explanation**: Numba's JIT (Just-In-Time) compiler translates Python functions to optimized machine code when first called, achieving C-like performance.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Profile first**: Don't guess where the bottleneck is\n",
    "2. **Algorithm matters**: O(n) vs O(nÂ²) beats any micro-optimization\n",
    "3. **Use the right tool**: NumPy for arrays, Numba for loops, asyncio for I/O\n",
    "4. **Memory is finite**: Use generators and streaming for large data\n",
    "5. **GIL awareness**: Multiprocessing for CPU, threading for I/O\n",
    "6. **Vectorization wins**: Avoid Python loops on arrays when possible\n",
    "7. **Measure everything**: Benchmark before and after optimization\n",
    "8. **Premature optimization**: Focus on correctness first, then optimize\n",
    "9. **Compiled beats interpreted**: Numba/Cython for critical hot paths\n",
    "10. **Concurrency â‰  Parallelism**: Understand the difference\n",
    "\n",
    "---\n",
    "\n",
    "## Common Mistakes to Avoid\n",
    "\n",
    "1. **Optimizing before profiling** - Wasting time on non-bottlenecks\n",
    "2. **Using threads for CPU-bound work** - GIL prevents speedup\n",
    "3. **Loops on NumPy arrays** - Defeats vectorization benefits\n",
    "4. **Blocking asyncio event loop** - CPU work in async functions\n",
    "5. **Premature multiprocessing** - Process overhead can be worse\n",
    "6. **Ignoring memory usage** - OOM kills vs slow execution\n",
    "7. **Not caching expensive calls** - Repeated work\n",
    "8. **String concatenation in loops** - O(nÂ²) behavior\n",
    "9. **Wrong data structure** - list vs set for membership testing\n",
    "10. **Not testing at scale** - Small data hides problems\n",
    "\n",
    "---\n",
    "\n",
    "## Pro Tips\n",
    "\n",
    "1. **Use cProfile + SnakeViz**: Visual profiling is enlightening\n",
    "2. **Try PyPy**: 5-10x speedup for long-running pure Python code\n",
    "3. **Learn NumPy broadcasting**: Eliminates many loops\n",
    "4. **Cache expensive I/O**: Redis, memcached, or functools.lru_cache\n",
    "5. **Use binary formats**: Pickle, HDF5, Parquet beat JSON/CSV\n",
    "6. **Batch operations**: Database queries, API calls\n",
    "7. **Measure memory: sys.getsizeof(), memory_profiler\n",
    "8. **Use __slots__**: For classes with many instances\n",
    "9. **Compile regexes**: re.compile() once, use many times\n",
    "10. **Monitor production**: Profile in real environments\n",
    "\n",
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "**You've mastered high-performance Python!** Next topics:\n",
    "\n",
    "1. **GPU Computing**: CUDA, CuPy, JAX (next notebook!)\n",
    "2. **Distributed Computing**: Dask, Ray, Spark\n",
    "3. **Advanced Profiling**: perf, valgrind, flame graphs\n",
    "4. **C Extensions**: ctypes, CFFI, Cython mastery\n",
    "5. **Alternative Runtimes**: PyPy, GraalPython\n",
    "\n",
    "**Practice Projects:**\n",
    "- Optimize a real application (10x+ speedup possible!)\n",
    "- Build a parallel data processing pipeline\n",
    "- Create a high-performance web service\n",
    "- Implement numerical algorithms in Numba\n",
    "\n",
    "**Remember**: Fast code is good. Correct code is better. Maintainable code is best. Optimize where it matters!\n",
    "\n",
    "**Ready for GPU computing? Proceed to the CUDA & Parallel Computing notebook!** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
