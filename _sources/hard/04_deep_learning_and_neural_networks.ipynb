{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Deep Learning and Neural Networks\n",
    "\n",
    "**Master the fundamentals of deep learning and build production-ready neural networks**\n",
    "\n",
    "## Real-World Context\n",
    "Deep learning powers modern AI systems from GPT-4 to self-driving cars. Understanding neural networks is essential for any ML engineer - they're used in computer vision (Tesla Autopilot), natural language processing (ChatGPT), speech recognition (Siri), recommendation systems (Netflix), and countless other applications.\n",
    "\n",
    "## What You'll Learn\n",
    "1. **Neural Network Theory**: Neurons, layers, activation functions, and forward propagation\n",
    "2. **Backpropagation**: How networks learn through gradient descent\n",
    "3. **Optimizers**: Adam, SGD, RMSprop and their trade-offs\n",
    "4. **Regularization**: Dropout, batch normalization, L1/L2 regularization\n",
    "5. **Convolutional Neural Networks (CNNs)**: Architecture for computer vision\n",
    "6. **Advanced Architectures**: ResNets, Inception, attention mechanisms\n",
    "7. **Training Strategies**: Learning rate schedules, callbacks, early stopping\n",
    "8. **Transfer Learning**: Leveraging pre-trained models\n",
    "9. **Production Best Practices**: Model saving, versioning, deployment\n",
    "\n",
    "**Prerequisites**: Python, NumPy, basic machine learning concepts\n",
    "\n",
    "**Time**: 3-4 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Neural Network Fundamentals\n",
    "\n",
    "### What is a Neural Network?\n",
    "\n",
    "A neural network is a computational model inspired by the human brain:\n",
    "\n",
    "- **Biological neurons**: Receive signals through dendrites, process in cell body, output through axon\n",
    "- **Artificial neurons**: Receive inputs, apply weights, add bias, pass through activation function\n",
    "\n",
    "### Architecture Components\n",
    "\n",
    "| Component | Purpose | Example |\n",
    "|-----------|---------|----------|\n",
    "| **Input Layer** | Receives raw data | 784 pixels for MNIST images |\n",
    "| **Hidden Layers** | Extract features | [128, 64, 32] neurons in 3 layers |\n",
    "| **Output Layer** | Produces predictions | 10 neurons for digit classification |\n",
    "| **Weights** | Learnable parameters | Matrix of connections between layers |\n",
    "| **Biases** | Learnable offsets | One per neuron |\n",
    "| **Activation Functions** | Introduce non-linearity | ReLU, sigmoid, tanh |\n",
    "\n",
    "### The Mathematics\n",
    "\n",
    "For a single neuron:\n",
    "```\n",
    "z = Œ£(wi √ó xi) + b     # Linear combination\n",
    "a = œÉ(z)               # Activation function\n",
    "```\n",
    "\n",
    "For a layer:\n",
    "```\n",
    "Z = W √ó X + b          # Matrix multiplication\n",
    "A = œÉ(Z)               # Element-wise activation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity, allowing networks to learn complex patterns.\n",
    "\n",
    "### Common Activation Functions\n",
    "\n",
    "| Function | Formula | Range | Use Case | Pros | Cons |\n",
    "|----------|---------|-------|----------|------|------|\n",
    "| **ReLU** | max(0, x) | [0, ‚àû) | Hidden layers | Fast, no vanishing gradient | Dead neurons |\n",
    "| **Leaky ReLU** | max(0.01x, x) | (-‚àû, ‚àû) | Hidden layers | Fixes dead ReLU | Slightly slower |\n",
    "| **Sigmoid** | 1/(1+e^-x) | (0, 1) | Binary output | Probabilistic interpretation | Vanishing gradient |\n",
    "| **Tanh** | (e^x - e^-x)/(e^x + e^-x) | (-1, 1) | Hidden layers (RNN) | Zero-centered | Vanishing gradient |\n",
    "| **Softmax** | e^xi / Œ£e^xj | [0, 1], sum=1 | Multi-class output | Probability distribution | N/A |\n",
    "| **Swish** | x √ó sigmoid(x) | (-‚àû, ‚àû) | Modern architectures | Smooth, self-gated | More computation |\n",
    "\n",
    "### Why Non-Linearity Matters\n",
    "Without activation functions, deep networks collapse to a single linear transformation:\n",
    "```\n",
    "Layer 1: y = W1√óx\n",
    "Layer 2: z = W2√óy = W2√ó(W1√óx) = (W2√óW1)√óx = W_combined√óx\n",
    "```\n",
    "This defeats the purpose of depth!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed):\n",
    "# !pip install tensorflow numpy matplotlib scikit-learn pandas seaborn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation functions\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def swish(x):\n",
    "    return x * sigmoid(x)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "functions = [\n",
    "    (sigmoid, 'Sigmoid', 'purple'),\n",
    "    (tanh, 'Tanh', 'blue'),\n",
    "    (relu, 'ReLU', 'red'),\n",
    "    (leaky_relu, 'Leaky ReLU', 'orange'),\n",
    "    (swish, 'Swish', 'green'),\n",
    "]\n",
    "\n",
    "for i, (func, name, color) in enumerate(functions):\n",
    "    axes[i].plot(x, func(x), color=color, linewidth=2)\n",
    "    axes[i].set_title(name, fontsize=12, fontweight='bold')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].axhline(y=0, color='black', linewidth=0.5)\n",
    "    axes[i].axvline(x=0, color='black', linewidth=0.5)\n",
    "    axes[i].set_xlabel('Input')\n",
    "    axes[i].set_ylabel('Output')\n",
    "\n",
    "# Hide last subplot\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Key Observations:\")\n",
    "print(\"  - ReLU: Zero for negative, linear for positive (most popular)\")\n",
    "print(\"  - Sigmoid: Saturates at 0 and 1 (use for probabilities)\")\n",
    "print(\"  - Tanh: Zero-centered version of sigmoid\")\n",
    "print(\"  - Leaky ReLU: Prevents 'dead neurons' with small negative slope\")\n",
    "print(\"  - Swish: Smooth, self-gated (used in EfficientNet)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building Your First Neural Network\n",
    "\n",
    "Let's build a network to classify non-linear data that a linear classifier can't handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate non-linear dataset (two interleaving half circles)\n",
    "X, y = make_moons(n_samples=1000, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features (important for neural networks!)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], \n",
    "            c='blue', label='Class 0', alpha=0.6, edgecolors='black')\n",
    "plt.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], \n",
    "            c='red', label='Class 1', alpha=0.6, edgecolors='black')\n",
    "plt.title('Non-Linear Classification Problem', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")\n",
    "print(f\"Feature shape: {X_train.shape[1]}\")\n",
    "print(f\"Class distribution: {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build neural network\n",
    "model = keras.Sequential([\n",
    "    # Input layer (implicitly defined by first layer)\n",
    "    layers.Dense(64, activation='relu', input_shape=(2,), name='hidden1'),\n",
    "    layers.Dropout(0.2),  # Regularization: randomly drop 20% of neurons during training\n",
    "    \n",
    "    layers.Dense(32, activation='relu', name='hidden2'),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Dense(16, activation='relu', name='hidden3'),\n",
    "    \n",
    "    # Output layer\n",
    "    layers.Dense(1, activation='sigmoid', name='output')  # Binary classification\n",
    "], name='simple_nn')\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',  # Adaptive learning rate optimizer\n",
    "    loss='binary_crossentropy',  # For binary classification\n",
    "    metrics=['accuracy']  # Track accuracy during training\n",
    ")\n",
    "\n",
    "# Display architecture\n",
    "model.summary()\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params = model.count_params()\n",
    "print(f\"\\nüìä Total trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Architecture\n",
    "\n",
    "**Layer Sizes:**\n",
    "- Input: 2 features\n",
    "- Hidden 1: 64 neurons ‚Üí 2√ó64 + 64 bias = 192 params\n",
    "- Hidden 2: 32 neurons ‚Üí 64√ó32 + 32 bias = 2,080 params\n",
    "- Hidden 3: 16 neurons ‚Üí 32√ó16 + 16 bias = 528 params\n",
    "- Output: 1 neuron ‚Üí 16√ó1 + 1 bias = 17 params\n",
    "- **Total: 2,817 parameters**\n",
    "\n",
    "**Why this architecture?**\n",
    "- **Funnel shape** (64‚Üí32‚Üí16): Common pattern for classification\n",
    "- **Dropout layers**: Prevent overfitting by randomly disabling neurons\n",
    "- **ReLU activation**: Fast, effective, avoids vanishing gradients\n",
    "- **Sigmoid output**: Produces probability between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with validation\n",
    "print(\"üèãÔ∏è Training neural network...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,  # Process 32 samples at a time\n",
    "    validation_split=0.2,  # Use 20% of training data for validation\n",
    "    verbose=0  # Suppress epoch-by-epoch output\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"‚úÖ Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"üìâ Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Final training accuracy\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "print(f\"\\nüìä Final Training Accuracy: {final_train_acc * 100:.2f}%\")\n",
    "print(f\"üìä Final Validation Accuracy: {final_val_acc * 100:.2f}%\")\n",
    "\n",
    "# Check for overfitting\n",
    "if final_train_acc - final_val_acc > 0.05:\n",
    "    print(\"‚ö†Ô∏è  Warning: Potential overfitting detected (train-val gap > 5%)\")\n",
    "else:\n",
    "    print(\"‚úÖ No significant overfitting detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy\n",
    "ax1.plot(history.history['accuracy'], label='Training', linewidth=2)\n",
    "ax1.plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "ax1.set_title('Model Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "ax2.plot(history.history['loss'], label='Training', linewidth=2, color='red')\n",
    "ax2.plot(history.history['val_loss'], label='Validation', linewidth=2, color='orange')\n",
    "ax2.set_title('Model Loss Over Time', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà Interpretation:\")\n",
    "print(\"  - Both training and validation curves should decrease\")\n",
    "print(\"  - Gap between curves indicates overfitting\")\n",
    "print(\"  - Validation loss increasing = model memorizing, not learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Backpropagation and Gradient Descent\n",
    "\n",
    "### How Neural Networks Learn\n",
    "\n",
    "**Forward Pass (Prediction):**\n",
    "```\n",
    "Input ‚Üí Layer 1 ‚Üí Layer 2 ‚Üí ... ‚Üí Output ‚Üí Loss\n",
    "```\n",
    "\n",
    "**Backward Pass (Learning):**\n",
    "```\n",
    "Loss ‚Üí ‚àÇLoss/‚àÇweights ‚Üí Update weights ‚Üê Gradient descent\n",
    "```\n",
    "\n",
    "### Gradient Descent Variants\n",
    "\n",
    "| Optimizer | Learning Rate | Memory | Speed | Use Case |\n",
    "|-----------|---------------|--------|-------|----------|\n",
    "| **SGD** | Constant | Low | Slow | Simple problems |\n",
    "| **SGD + Momentum** | Constant + velocity | Low | Medium | Helps escape local minima |\n",
    "| **RMSprop** | Adaptive per-parameter | Medium | Fast | Recurrent networks |\n",
    "| **Adam** | Adaptive + momentum | High | Fast | Default choice (most tasks) |\n",
    "| **AdamW** | Adam + weight decay | High | Fast | Modern SOTA (transformers) |\n",
    "\n",
    "### The Mathematics\n",
    "\n",
    "**Standard gradient descent:**\n",
    "```\n",
    "w_new = w_old - learning_rate √ó ‚àÇLoss/‚àÇw\n",
    "```\n",
    "\n",
    "**Adam (simplified):**\n",
    "```\n",
    "m_t = Œ≤1 √ó m_{t-1} + (1-Œ≤1) √ó gradient          # First moment (momentum)\n",
    "v_t = Œ≤2 √ó v_{t-1} + (1-Œ≤2) √ó gradient¬≤         # Second moment (variance)\n",
    "w_new = w_old - learning_rate √ó m_t / ‚àö(v_t)    # Update\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different optimizers\n",
    "optimizers_to_test = [\n",
    "    ('SGD', keras.optimizers.SGD(learning_rate=0.01)),\n",
    "    ('SGD+Momentum', keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)),\n",
    "    ('RMSprop', keras.optimizers.RMSprop(learning_rate=0.001)),\n",
    "    ('Adam', keras.optimizers.Adam(learning_rate=0.001)),\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, optimizer in optimizers_to_test:\n",
    "    print(f\"\\nüîÑ Training with {name}...\")\n",
    "    \n",
    "    # Create fresh model\n",
    "    test_model = keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(2,)),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    test_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train\n",
    "    hist = test_model.fit(X_train, y_train, epochs=30, batch_size=32, \n",
    "                          validation_split=0.2, verbose=0)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = hist.history['val_accuracy']\n",
    "    final_acc = hist.history['val_accuracy'][-1]\n",
    "    print(f\"  ‚úÖ Final validation accuracy: {final_acc * 100:.2f}%\")\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, accuracies in results.items():\n",
    "    plt.plot(accuracies, label=name, linewidth=2)\n",
    "\n",
    "plt.title('Optimizer Comparison: Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Key Insights:\")\n",
    "print(\"  - Adam typically converges fastest\")\n",
    "print(\"  - SGD with momentum is more stable than plain SGD\")\n",
    "print(\"  - RMSprop works well but Adam is usually better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Regularization Techniques\n",
    "\n",
    "Regularization prevents overfitting by constraining model complexity.\n",
    "\n",
    "### Common Regularization Methods\n",
    "\n",
    "| Technique | How it Works | When to Use | Strength |\n",
    "|-----------|--------------|-------------|----------|\n",
    "| **Dropout** | Randomly disable neurons | Most cases | 0.2-0.5 |\n",
    "| **L2 Regularization** | Penalize large weights | Small datasets | 0.001-0.01 |\n",
    "| **L1 Regularization** | Penalize non-zero weights | Feature selection | 0.001-0.01 |\n",
    "| **Batch Normalization** | Normalize layer inputs | Deep networks | N/A |\n",
    "| **Early Stopping** | Stop when validation plateaus | Always | patience=5-10 |\n",
    "| **Data Augmentation** | Generate variations | Images, text | N/A |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with aggressive regularization\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "regularized_model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(2,),\n",
    "                kernel_regularizer=regularizers.l2(0.01)),  # L2 regularization\n",
    "    layers.BatchNormalization(),  # Normalize activations\n",
    "    layers.Dropout(0.3),  # Drop 30% of neurons\n",
    "    \n",
    "    layers.Dense(32, activation='relu',\n",
    "                kernel_regularizer=regularizers.l2(0.01)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "], name='regularized_model')\n",
    "\n",
    "regularized_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Watch validation loss\n",
    "    patience=10,  # Stop if no improvement for 10 epochs\n",
    "    restore_best_weights=True,  # Revert to best model\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train with regularization\n",
    "reg_history = regularized_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,  # More epochs, but early stopping will halt training\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Training stopped after {len(reg_history.history['loss'])} epochs\")\n",
    "test_loss, test_acc = regularized_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"‚úÖ Test Accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Convolutional Neural Networks (CNNs)\n",
    "\n",
    "CNNs are specialized for **spatial data** (images, video, audio spectrograms).\n",
    "\n",
    "### Why CNNs for Images?\n",
    "\n",
    "**Traditional neural networks:**\n",
    "- 28√ó28 image ‚Üí 784 input neurons\n",
    "- 224√ó224 RGB image ‚Üí 150,528 input neurons!\n",
    "- Doesn't exploit spatial structure\n",
    "- Too many parameters\n",
    "\n",
    "**CNNs solve this:**\n",
    "- **Local connectivity**: Each neuron sees small region (receptive field)\n",
    "- **Parameter sharing**: Same filter used across entire image\n",
    "- **Translation invariance**: Detect features anywhere in image\n",
    "\n",
    "### CNN Layers\n",
    "\n",
    "| Layer | Purpose | Parameters | Output |\n",
    "|-------|---------|------------|--------|\n",
    "| **Conv2D** | Detect features | Filters (e.g., 3√ó3√ó32) | Feature maps |\n",
    "| **MaxPooling2D** | Downsample | None | Reduced spatial size |\n",
    "| **BatchNormalization** | Stabilize training | Œ≥, Œ≤ per channel | Normalized activations |\n",
    "| **Flatten** | Convert to 1D | None | Vector |\n",
    "| **Dense** | Classification | Weights + biases | Predictions |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset (handwritten digits)\n",
    "print(\"üì¶ Loading MNIST dataset...\\n\")\n",
    "(X_train_img, y_train_img), (X_test_img, y_test_img) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess\n",
    "X_train_img = X_train_img.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "X_test_img = X_test_img.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode labels (0-9 ‚Üí 10 binary columns)\n",
    "y_train_img = keras.utils.to_categorical(y_train_img, 10)\n",
    "y_test_img = keras.utils.to_categorical(y_test_img, 10)\n",
    "\n",
    "print(f\"Training images: {X_train_img.shape}\")\n",
    "print(f\"Test images: {X_test_img.shape}\")\n",
    "print(f\"Image shape: {X_train_img.shape[1:]}\")\n",
    "print(f\"Number of classes: {y_train_img.shape[1]}\")\n",
    "\n",
    "# Visualize samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_train_img[i].squeeze(), cmap='gray')\n",
    "    label = np.argmax(y_train_img[i])\n",
    "    ax.set_title(f'Label: {label}', fontsize=12)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Sample MNIST Images', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN architecture\n",
    "cnn_model = keras.Sequential([\n",
    "    # First convolutional block\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), padding='same'),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),  # 28√ó28 ‚Üí 14√ó14\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    # Second convolutional block\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),  # 14√ó14 ‚Üí 7√ó7\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    # Third convolutional block\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    # Flatten and dense layers\n",
    "    layers.Flatten(),  # 7√ó7√ó128 = 6,272 features\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')  # 10 digit classes\n",
    "], name='cnn_mnist')\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "cnn_model.summary()\n",
    "print(f\"\\nüìä Total parameters: {cnn_model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN with callbacks\n",
    "print(\"üèãÔ∏è Training CNN...\\n\")\n",
    "\n",
    "# Learning rate reduction on plateau\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,  # Reduce LR by half\n",
    "    patience=3,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "early_stop_cnn = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train (using subset for speed in demo)\n",
    "cnn_history = cnn_model.fit(\n",
    "    X_train_img[:20000], y_train_img[:20000],\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[reduce_lr, early_stop_cnn],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = cnn_model.evaluate(X_test_img, y_test_img, verbose=0)\n",
    "print(f\"\\n‚úÖ Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "print(f\"üìâ Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CNN predictions\n",
    "predictions = cnn_model.predict(X_test_img[:20], verbose=0)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test_img[:20], axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(4, 5, figsize=(15, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_test_img[i].squeeze(), cmap='gray')\n",
    "    pred = predicted_classes[i]\n",
    "    true = true_classes[i]\n",
    "    confidence = predictions[i][pred] * 100\n",
    "    \n",
    "    color = 'green' if pred == true else 'red'\n",
    "    ax.set_title(f'True: {true}\\nPred: {pred} ({confidence:.1f}%)', \n",
    "                fontsize=10, color=color, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('CNN Predictions (Green=Correct, Red=Wrong)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "all_preds = np.argmax(cnn_model.predict(X_test_img, verbose=0), axis=1)\n",
    "all_true = np.argmax(y_test_img, axis=1)\n",
    "cm = confusion_matrix(all_true, all_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Transfer Learning\n",
    "\n",
    "Transfer learning leverages pre-trained models to solve new tasks faster with less data.\n",
    "\n",
    "### Why Transfer Learning?\n",
    "\n",
    "**Training from scratch:**\n",
    "- Requires millions of images\n",
    "- Needs days/weeks on GPUs\n",
    "- Expensive ($1000s in compute)\n",
    "\n",
    "**Transfer learning:**\n",
    "- Use model trained on ImageNet (14M images, 1000 classes)\n",
    "- Fine-tune on your dataset (can be just 100s of images)\n",
    "- Train in hours, not days\n",
    "\n",
    "### Popular Pre-trained Models\n",
    "\n",
    "| Model | Parameters | Top-1 Acc | Year | Use Case |\n",
    "|-------|------------|-----------|------|----------|\n",
    "| **VGG16** | 138M | 71.3% | 2014 | Simple, interpretable |\n",
    "| **ResNet50** | 25.6M | 76.1% | 2015 | Good balance |\n",
    "| **Inception-v3** | 23.9M | 77.9% | 2015 | Multi-scale features |\n",
    "| **MobileNetV2** | 3.5M | 71.8% | 2018 | Mobile/embedded |\n",
    "| **EfficientNet-B0** | 5.3M | 77.1% | 2019 | Best accuracy/size |\n",
    "| **Vision Transformer** | 86M | 84.2% | 2020 | SOTA (expensive) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet50 (without top classification layer)\n",
    "print(\"üì¶ Loading pre-trained ResNet50...\\n\")\n",
    "\n",
    "base_model = keras.applications.ResNet50(\n",
    "    weights='imagenet',  # Pre-trained on ImageNet\n",
    "    include_top=False,  # Exclude final classification layer\n",
    "    input_shape=(224, 224, 3)  # Standard ImageNet size\n",
    ")\n",
    "\n",
    "# Freeze base model layers (don't train them initially)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom classification head\n",
    "transfer_model = keras.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),  # Reduce to 1D\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')  # 10 classes (example)\n",
    "], name='transfer_learning_model')\n",
    "\n",
    "transfer_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"Total parameters: {transfer_model.count_params():,}\")\n",
    "print(f\"Trainable parameters: {sum([tf.size(w).numpy() for w in transfer_model.trainable_weights]):,}\")\n",
    "print(f\"Non-trainable parameters: {sum([tf.size(w).numpy() for w in transfer_model.non_trainable_weights]):,}\")\n",
    "\n",
    "print(\"\\nüéØ Transfer Learning Strategy:\")\n",
    "print(\"  1. Freeze pre-trained layers (use learned features)\")\n",
    "print(\"  2. Train only custom classification head\")\n",
    "print(\"  3. Optionally: Unfreeze top layers and fine-tune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Strategy\n",
    "\n",
    "**Phase 1: Train head only**\n",
    "```python\n",
    "base_model.trainable = False\n",
    "model.fit(...)  # Train for 5-10 epochs\n",
    "```\n",
    "\n",
    "**Phase 2: Fine-tune top layers**\n",
    "```python\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-20]:  # Freeze all but last 20 layers\n",
    "    layer.trainable = False\n",
    "model.fit(..., learning_rate=1e-5)  # Lower LR!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Advanced Architectures\n",
    "\n",
    "### Residual Connections (ResNet)\n",
    "\n",
    "**Problem**: Deep networks suffer from vanishing gradients\n",
    "\n",
    "**Solution**: Skip connections that allow gradients to flow directly\n",
    "\n",
    "```\n",
    "Traditional:  x ‚Üí Conv ‚Üí Conv ‚Üí y\n",
    "Residual:     x ‚Üí Conv ‚Üí Conv ‚Üí (+) ‚Üí y\n",
    "              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò (skip connection)\n",
    "```\n",
    "\n",
    "**Mathematics:**\n",
    "```\n",
    "y = F(x) + x    # Instead of y = F(x)\n",
    "```\n",
    "\n",
    "This allows networks with 100+ layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a residual block\n",
    "def residual_block(x, filters, kernel_size=3, stride=1):\n",
    "    \"\"\"Create a residual block with skip connection.\"\"\"\n",
    "    # Main path\n",
    "    y = layers.Conv2D(filters, kernel_size, strides=stride, padding='same')(x)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Activation('relu')(y)\n",
    "    y = layers.Conv2D(filters, kernel_size, strides=1, padding='same')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    \n",
    "    # Skip connection (adjust dimensions if needed)\n",
    "    if stride != 1 or x.shape[-1] != filters:\n",
    "        x = layers.Conv2D(filters, 1, strides=stride, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Add skip connection\n",
    "    out = layers.Add()([x, y])\n",
    "    out = layers.Activation('relu')(out)\n",
    "    \n",
    "    return out\n",
    "\n",
    "# Build mini-ResNet\n",
    "def build_mini_resnet(input_shape=(28, 28, 1), num_classes=10):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Initial convolution\n",
    "    x = layers.Conv2D(32, 3, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    # Residual blocks\n",
    "    x = residual_block(x, 32)\n",
    "    x = residual_block(x, 64, stride=2)  # Downsample\n",
    "    x = residual_block(x, 64)\n",
    "    x = residual_block(x, 128, stride=2)  # Downsample\n",
    "    x = residual_block(x, 128)\n",
    "    \n",
    "    # Classification head\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=outputs, name='mini_resnet')\n",
    "\n",
    "resnet_model = build_mini_resnet()\n",
    "resnet_model.summary()\n",
    "print(f\"\\nüìä Total parameters: {resnet_model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Mechanisms (Simplified)\n",
    "\n",
    "Attention allows the model to **focus on important features**.\n",
    "\n",
    "**Intuition**: When reading a sentence, you don't give equal attention to every word.\n",
    "\n",
    "**Mathematics (simplified):**\n",
    "```\n",
    "Attention(Q, K, V) = softmax(Q¬∑K^T / ‚àöd) ¬∑ V\n",
    "```\n",
    "\n",
    "Where:\n",
    "- Q = Query (what we're looking for)\n",
    "- K = Key (what's available)\n",
    "- V = Value (actual information)\n",
    "\n",
    "Used in:\n",
    "- Transformers (GPT, BERT)\n",
    "- Vision Transformers (ViT)\n",
    "- Multi-modal models (CLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple channel attention (Squeeze-and-Excitation)\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "    \"\"\"\n",
    "    Channel attention: Which feature maps are important?\n",
    "    \"\"\"\n",
    "    channel = input_feature.shape[-1]\n",
    "    \n",
    "    # Squeeze: Global average pooling\n",
    "    x = layers.GlobalAveragePooling2D()(input_feature)\n",
    "    \n",
    "    # Excitation: Learn channel importance\n",
    "    x = layers.Dense(channel // ratio, activation='relu')(x)\n",
    "    x = layers.Dense(channel, activation='sigmoid')(x)\n",
    "    \n",
    "    # Reshape and multiply\n",
    "    x = layers.Reshape((1, 1, channel))(x)\n",
    "    return layers.Multiply()([input_feature, x])\n",
    "\n",
    "# Example: Add attention to a CNN\n",
    "inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(32, 3, padding='same', activation='relu')(inputs)\n",
    "x = channel_attention(x)  # ‚Üê Add attention!\n",
    "x = layers.MaxPooling2D()(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "attention_model = keras.Model(inputs, outputs, name='cnn_with_attention')\n",
    "print(\"‚úÖ CNN with channel attention created!\")\n",
    "print(f\"üìä Parameters: {attention_model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Training Best Practices\n",
    "\n",
    "### Learning Rate Schedules\n",
    "\n",
    "| Strategy | Description | Use Case |\n",
    "|----------|-------------|----------|\n",
    "| **Constant** | Same LR throughout | Simple problems |\n",
    "| **Step Decay** | Reduce LR every N epochs | General purpose |\n",
    "| **Exponential Decay** | LR = LR‚ÇÄ √ó e^(-kt) | Smooth reduction |\n",
    "| **Cosine Annealing** | Follows cosine curve | SOTA training |\n",
    "| **ReduceLROnPlateau** | Reduce when stuck | Adaptive |\n",
    "| **One Cycle** | Increase then decrease | Fast training |\n",
    "\n",
    "### Data Augmentation (Images)\n",
    "\n",
    "Artificially expand dataset by applying transformations:\n",
    "- Random rotation (¬±15¬∞)\n",
    "- Horizontal/vertical flip\n",
    "- Zoom (90%-110%)\n",
    "- Shift (¬±10%)\n",
    "- Brightness/contrast\n",
    "- Cutout/mixup (advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate schedules\n",
    "import math\n",
    "\n",
    "# Cosine annealing\n",
    "def cosine_annealing(epoch, lr, total_epochs=50, min_lr=1e-6):\n",
    "    \"\"\"Cosine annealing learning rate schedule.\"\"\"\n",
    "    return min_lr + (lr - min_lr) * (1 + math.cos(math.pi * epoch / total_epochs)) / 2\n",
    "\n",
    "# Visualize schedule\n",
    "epochs = np.arange(50)\n",
    "initial_lr = 0.001\n",
    "lrs = [cosine_annealing(e, initial_lr) for e in epochs]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, lrs, linewidth=2, color='blue')\n",
    "plt.title('Cosine Annealing Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Benefits of LR scheduling:\")\n",
    "print(\"  - Start high: Explore loss landscape quickly\")\n",
    "print(\"  - End low: Fine-tune to optimal solution\")\n",
    "print(\"  - Cosine: Smooth transitions, no sudden jumps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation example\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create augmentation pipeline\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,  # Random rotation ¬±15¬∞\n",
    "    width_shift_range=0.1,  # Horizontal shift ¬±10%\n",
    "    height_shift_range=0.1,  # Vertical shift ¬±10%\n",
    "    zoom_range=0.1,  # Zoom 90%-110%\n",
    "    shear_range=0.1,  # Shear transformation\n",
    "    fill_mode='nearest'  # Fill empty pixels\n",
    ")\n",
    "\n",
    "# Visualize augmented images\n",
    "sample_img = X_train_img[0:1]  # Take first image\n",
    "sample_label = y_train_img[0:1]\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Original\n",
    "axes[0].imshow(sample_img[0].squeeze(), cmap='gray')\n",
    "axes[0].set_title('Original', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Augmented versions\n",
    "for i, batch in enumerate(datagen.flow(sample_img, batch_size=1)):\n",
    "    if i >= 9:\n",
    "        break\n",
    "    axes[i+1].imshow(batch[0].squeeze(), cmap='gray')\n",
    "    axes[i+1].set_title(f'Augmented {i+1}', fontsize=12)\n",
    "    axes[i+1].axis('off')\n",
    "\n",
    "plt.suptitle('Data Augmentation Examples', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéØ Data Augmentation Benefits:\")\n",
    "print(\"  - Reduces overfitting (model sees variations)\")\n",
    "print(\"  - Acts as regularization\")\n",
    "print(\"  - Improves generalization to new data\")\n",
    "print(\"  - Effective with small datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Model Deployment\n",
    "\n",
    "### Saving and Loading Models\n",
    "\n",
    "**Three formats:**\n",
    "\n",
    "| Format | Extension | Use Case | Size | Load Speed |\n",
    "|--------|-----------|----------|------|------------|\n",
    "| **SavedModel** | / (directory) | Production (TF Serving) | Large | Medium |\n",
    "| **HDF5** | .h5, .keras | Development | Medium | Fast |\n",
    "| **TFLite** | .tflite | Mobile/Edge | Small | Very fast |\n",
    "| **ONNX** | .onnx | Cross-platform | Medium | Fast |\n",
    "\n",
    "### Production Checklist\n",
    "\n",
    "- ‚úÖ Save model architecture and weights\n",
    "- ‚úÖ Save preprocessing parameters (scaler, tokenizer)\n",
    "- ‚úÖ Version control (model_v1, model_v2, ...)\n",
    "- ‚úÖ Document input/output shapes and types\n",
    "- ‚úÖ Test on validation set\n",
    "- ‚úÖ Benchmark inference time\n",
    "- ‚úÖ Monitor performance in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model (multiple formats)\n",
    "import os\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "# 1. SavedModel format (TensorFlow native)\n",
    "cnn_model.save('saved_models/cnn_mnist')\n",
    "print(\"‚úÖ Saved: SavedModel format (directory)\")\n",
    "\n",
    "# 2. HDF5 format (legacy, but widely used)\n",
    "cnn_model.save('saved_models/cnn_mnist.h5')\n",
    "print(\"‚úÖ Saved: HDF5 format (.h5)\")\n",
    "\n",
    "# 3. Keras format (recommended for Keras 3+)\n",
    "cnn_model.save('saved_models/cnn_mnist.keras')\n",
    "print(\"‚úÖ Saved: Keras format (.keras)\")\n",
    "\n",
    "# Save weights only (smaller file)\n",
    "cnn_model.save_weights('saved_models/cnn_mnist_weights.h5')\n",
    "print(\"‚úÖ Saved: Weights only (.h5)\")\n",
    "\n",
    "print(\"\\nüìÇ Saved model files:\")\n",
    "for root, dirs, files in os.walk('saved_models'):\n",
    "    for file in files:\n",
    "        path = os.path.join(root, file)\n",
    "        size = os.path.getsize(path) / 1024  # KB\n",
    "        print(f\"  {file}: {size:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "loaded_model = keras.models.load_model('saved_models/cnn_mnist.keras')\n",
    "print(\"‚úÖ Model loaded successfully!\\n\")\n",
    "\n",
    "# Verify it works\n",
    "test_loss, test_acc = loaded_model.evaluate(X_test_img[:1000], y_test_img[:1000], verbose=0)\n",
    "print(f\"Loaded model accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# Model versioning example\n",
    "import datetime\n",
    "version = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "versioned_path = f'saved_models/cnn_mnist_v{version}.keras'\n",
    "cnn_model.save(versioned_path)\n",
    "print(f\"\\n‚úÖ Versioned model saved: {versioned_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Common Mistakes and How to Avoid Them\n",
    "\n",
    "| Mistake | Symptom | Solution |\n",
    "|---------|---------|----------|\n",
    "| **Forgot to normalize data** | Poor accuracy | Scale inputs to [0,1] or standardize |\n",
    "| **Wrong activation on output** | NaN loss | Sigmoid for binary, softmax for multi-class |\n",
    "| **Too high learning rate** | Loss explodes | Start with 0.001 (Adam) or 0.01 (SGD) |\n",
    "| **Too small batch size** | Noisy training | Use 32-128 for most tasks |\n",
    "| **Not using validation set** | Can't detect overfitting | Always use validation_split or separate set |\n",
    "| **Forgetting dropout at test time** | Poor test accuracy | Use model.predict(), not training mode |\n",
    "| **Class imbalance** | Model predicts majority class | Use class weights or resampling |\n",
    "| **Vanishing gradients** | No learning in deep nets | Use ReLU, batch norm, residual connections |\n",
    "| **Data leakage** | Perfect val score, poor test | Normalize AFTER train/test split |\n",
    "| **Not shuffling data** | Poor generalization | Use shuffle=True in fit() |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Exercises\n",
    "\n",
    "### Exercise 1: CIFAR-10 CNN (‚≠ê‚≠ê)\n",
    "\n",
    "Build and train a CNN for CIFAR-10 dataset:\n",
    "1. Load CIFAR-10 (32√ó32 RGB images, 10 classes)\n",
    "2. Design CNN with at least 3 convolutional blocks\n",
    "3. Use data augmentation\n",
    "4. Apply learning rate scheduling and early stopping\n",
    "5. Achieve >70% test accuracy\n",
    "6. Visualize predictions and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Hint: keras.datasets.cifar10.load_data()\n",
    "# Hint: ImageDataGenerator for augmentation\n",
    "# Hint: Use reduce_lr and early_stop callbacks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Custom Activation Function (‚≠ê‚≠ê‚≠ê)\n",
    "\n",
    "Implement a custom activation function:\n",
    "1. Create a custom Mish activation: `x √ó tanh(ln(1 + e^x))`\n",
    "2. Build a network using your custom activation\n",
    "3. Compare performance with ReLU on MNIST\n",
    "4. Plot both activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Hint: @tf.function for custom activation\n",
    "# Hint: Use layers.Activation(custom_mish)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Transfer Learning (‚≠ê‚≠ê‚≠ê)\n",
    "\n",
    "Apply transfer learning to a small dataset:\n",
    "1. Use a subset of CIFAR-10 (only 1000 images)\n",
    "2. Load MobileNetV2 pre-trained on ImageNet\n",
    "3. Freeze base, train custom head\n",
    "4. Unfreeze top layers and fine-tune\n",
    "5. Compare accuracy with model trained from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "# Hint: keras.applications.MobileNetV2\n",
    "# Hint: Resize CIFAR-10 images to 96√ó96 or 224√ó224\n",
    "# Hint: Use very low LR for fine-tuning (1e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Build a ResNet Block (‚≠ê‚≠ê‚≠ê)\n",
    "\n",
    "Implement and test a ResNet architecture:\n",
    "1. Create a residual_block function with skip connections\n",
    "2. Build a small ResNet with 3-4 residual blocks\n",
    "3. Train on Fashion MNIST\n",
    "4. Compare with a regular CNN (same parameters)\n",
    "5. Visualize training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your code here\n",
    "# Hint: keras.datasets.fashion_mnist.load_data()\n",
    "# Hint: Use Functional API for skip connections\n",
    "# Hint: layers.Add()([shortcut, x])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Regularization Study (‚≠ê‚≠ê)\n",
    "\n",
    "Compare regularization techniques:\n",
    "1. Train 4 models on MNIST:\n",
    "   - No regularization\n",
    "   - Dropout only\n",
    "   - L2 regularization only\n",
    "   - Dropout + L2 + Batch Normalization\n",
    "2. Use small training set (5000 images)\n",
    "3. Plot training vs validation accuracy for all\n",
    "4. Identify which prevents overfitting best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Your code here\n",
    "# Hint: Use same architecture, vary regularization only\n",
    "# Hint: kernel_regularizer=regularizers.l2(0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Model Interpretation (‚≠ê‚≠ê‚≠ê‚≠ê)\n",
    "\n",
    "Visualize what a CNN learns:\n",
    "1. Train a CNN on MNIST\n",
    "2. Visualize first layer filters (convolutional kernels)\n",
    "3. Create activation maps for a test image\n",
    "4. Identify which filters activate for specific features\n",
    "5. **Bonus**: Implement Grad-CAM for class activation maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6: Your code here\n",
    "# Hint: model.layers[0].get_weights()[0] for filters\n",
    "# Hint: Create intermediate model: Model(inputs, layer.output)\n",
    "# Hint: Grad-CAM: gradient of output w.r.t. activations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Check Quiz\n",
    "\n",
    "Test your understanding:\n",
    "\n",
    "1. **Why do we need activation functions in neural networks?**\n",
    "   - A) To make training faster\n",
    "   - B) To introduce non-linearity\n",
    "   - C) To reduce overfitting\n",
    "   - D) To normalize outputs\n",
    "\n",
    "2. **Which optimizer is the default choice for most deep learning tasks?**\n",
    "   - A) SGD\n",
    "   - B) RMSprop\n",
    "   - C) Adam\n",
    "   - D) AdaGrad\n",
    "\n",
    "3. **What is the purpose of dropout?**\n",
    "   - A) Reduce model size\n",
    "   - B) Speed up training\n",
    "   - C) Prevent overfitting\n",
    "   - D) Improve accuracy\n",
    "\n",
    "4. **In a CNN, what does a convolutional layer do?**\n",
    "   - A) Classify images\n",
    "   - B) Detect local features\n",
    "   - C) Reduce dimensions\n",
    "   - D) Normalize inputs\n",
    "\n",
    "5. **What is transfer learning?**\n",
    "   - A) Training multiple models simultaneously\n",
    "   - B) Using pre-trained weights as initialization\n",
    "   - C) Transferring data between GPUs\n",
    "   - D) Converting models between frameworks\n",
    "\n",
    "6. **Which activation should be used for binary classification output?**\n",
    "   - A) ReLU\n",
    "   - B) Sigmoid\n",
    "   - C) Tanh\n",
    "   - D) Softmax\n",
    "\n",
    "7. **What is the main benefit of residual connections (ResNet)?**\n",
    "   - A) Fewer parameters\n",
    "   - B) Faster inference\n",
    "   - C) Solves vanishing gradient problem\n",
    "   - D) Better accuracy on small datasets\n",
    "\n",
    "8. **When should you normalize your data?**\n",
    "   - A) Before train/test split\n",
    "   - B) After train/test split\n",
    "   - C) Only for images\n",
    "   - D) Never\n",
    "\n",
    "9. **What is data augmentation?**\n",
    "   - A) Collecting more data\n",
    "   - B) Applying transformations to create variations\n",
    "   - C) Removing outliers\n",
    "   - D) Normalizing features\n",
    "\n",
    "10. **How do you detect overfitting?**\n",
    "    - A) High training loss\n",
    "    - B) Low test accuracy\n",
    "    - C) Large gap between train and validation accuracy\n",
    "    - D) Model trains too fast\n",
    "\n",
    "**Answers**: 1-B, 2-C, 3-C, 4-B, 5-B, 6-B, 7-C, 8-B, 9-B, 10-C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Architecture\n",
    "- ‚úÖ Neural networks learn hierarchical features through layers\n",
    "- ‚úÖ Activation functions introduce non-linearity (ReLU most common)\n",
    "- ‚úÖ Deeper networks can learn more complex patterns\n",
    "- ‚úÖ Skip connections (ResNets) enable very deep networks\n",
    "\n",
    "### Training\n",
    "- ‚úÖ Adam optimizer is the default choice for most tasks\n",
    "- ‚úÖ Always use validation set to detect overfitting\n",
    "- ‚úÖ Normalize inputs (critical for convergence)\n",
    "- ‚úÖ Learning rate scheduling improves final accuracy\n",
    "\n",
    "### Regularization\n",
    "- ‚úÖ Dropout prevents overfitting (0.2-0.5 typical)\n",
    "- ‚úÖ Batch normalization stabilizes training\n",
    "- ‚úÖ Data augmentation acts as regularization\n",
    "- ‚úÖ Early stopping prevents overtraining\n",
    "\n",
    "### CNNs\n",
    "- ‚úÖ CNNs exploit spatial structure in images\n",
    "- ‚úÖ Convolutional layers detect local features\n",
    "- ‚úÖ Pooling layers reduce spatial dimensions\n",
    "- ‚úÖ Transfer learning leverages pre-trained models\n",
    "\n",
    "### Production\n",
    "- ‚úÖ Save models with versioning\n",
    "- ‚úÖ Document input/output specifications\n",
    "- ‚úÖ Benchmark inference time\n",
    "- ‚úÖ Monitor performance in production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pro Tips\n",
    "\n",
    "1. **Start simple, then add complexity**: Begin with small network, add layers only if needed\n",
    "2. **Always visualize training curves**: Catch overfitting early\n",
    "3. **Use callbacks**: Early stopping, learning rate reduction, checkpointing\n",
    "4. **Normalize inputs**: Scale to [0,1] or standardize (Œº=0, œÉ=1)\n",
    "5. **Batch size matters**: 32-128 typical, larger = faster but less stable\n",
    "6. **Transfer learning for small datasets**: Don't train from scratch if you have <10k images\n",
    "7. **GPU makes 10-100√ó difference**: Use Colab/Kaggle for free GPUs\n",
    "8. **Read error messages carefully**: TensorFlow errors often suggest solutions\n",
    "9. **Version control your models**: Save each experiment with metadata\n",
    "10. **Stay up to date**: Deep learning evolves rapidly (follow papers/blogs)\n",
    "\n",
    "### Debugging Checklist\n",
    "- ‚ö†Ô∏è Loss is NaN ‚Üí Learning rate too high or wrong activation\n",
    "- ‚ö†Ô∏è Accuracy stuck at ~50% (binary) ‚Üí Model predicting one class\n",
    "- ‚ö†Ô∏è Training loss doesn't decrease ‚Üí Learning rate too low or data not normalized\n",
    "- ‚ö†Ô∏è Perfect train accuracy, poor validation ‚Üí Overfitting (add regularization)\n",
    "- ‚ö†Ô∏è Model trains very slowly ‚Üí Batch size too small or architecture too complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "### Continue in Hard Track:\n",
    "- **Lesson 5**: Advanced ML and NLP (transformers, BERT, GPT)\n",
    "- **Lesson 6**: Computer Systems and Theory\n",
    "- **Lesson 8**: Classic Problems (algorithms every engineer should know)\n",
    "\n",
    "### Deepen Your Knowledge:\n",
    "- **Stanford CS231n**: Convolutional Neural Networks for Visual Recognition\n",
    "- **Fast.ai**: Practical Deep Learning for Coders\n",
    "- **Deep Learning Book**: Goodfellow, Bengio, Courville\n",
    "- **Papers With Code**: Latest research implementations\n",
    "\n",
    "### Practice Projects:\n",
    "1. Image classification on your own dataset\n",
    "2. Object detection (YOLO, Faster R-CNN)\n",
    "3. Style transfer (neural artistic styles)\n",
    "4. GANs (generate realistic images)\n",
    "5. Deploy model to web app (Flask/FastAPI + TensorFlow.js)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now understand deep learning fundamentals and can build production-ready neural networks. Keep experimenting and building! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
